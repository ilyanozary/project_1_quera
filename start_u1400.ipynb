{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath ='U1400.xlsx'\n",
    "data98 = pd.read_excel(filepath, sheet_name= None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "p3_sheets = {\n",
    "    'U1400P3S01': 'Food',\n",
    "    'U1400P3S02': 'Tobacco',\n",
    "    'U1400P3S03': 'Clothing',\n",
    "    'U1400P3S04': 'Horsing',\n",
    "    'U1400P3S05': 'FrrniTrue',\n",
    "    'U1400P3S06': 'Health',\n",
    "    'U1400P3S07': 'Transport',\n",
    "    'U1400P3S08': 'Commrnication',\n",
    "    'U1400P3S09': 'recreation',\n",
    "    'U1400P3S11': 'Edrcation',\n",
    "    'U1400P3S12': 'Hotel',\n",
    "    'U1400P3S13': 'Miscellaneors',\n",
    "    'U1400P3S14': 'Investment'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sheet_name, expense_type in p3_sheets.items():\n",
    "    if sheet_name in data98:\n",
    "        columns_to_rename = {}\n",
    "\n",
    "\n",
    "        if 'code' in data98[sheet_name].columns:\n",
    "            columns_to_rename['code'] = f'{expense_type}_Code'\n",
    "\n",
    "        if 'prrchased' in data98[sheet_name].columns:\n",
    "            columns_to_rename['prrchased'] = f'{expense_type}_Prrchased'\n",
    "\n",
    "        if 'gram' in data98[sheet_name].columns:\n",
    "            columns_to_rename['gram'] = f'{expense_type}_Gram'\n",
    "\n",
    "        if 'kilogram' in data98[sheet_name].columns:\n",
    "            columns_to_rename['kilogram'] = f'{expense_type}_Kilogram'\n",
    "\n",
    "        if 'price' in data98[sheet_name].columns:\n",
    "            columns_to_rename['price'] = f'{expense_type}_Price'\n",
    "\n",
    "        if 'valre' in data98[sheet_name].columns:\n",
    "            columns_to_rename['valre'] = f'{expense_type}_Valre'\n",
    "\n",
    "        if 'mortgage' in data98[sheet_name].columns:\n",
    "            columns_to_rename['mortgage'] = f'{expense_type}_Mortgage'\n",
    "\n",
    "\n",
    "        data98[sheet_name] = data98[sheet_name].rename(columns=columns_to_rename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "U1400Data=pd.read_excel(filepath, sheet_name='U1400Data')\n",
    "U1400P1=pd.read_excel(filepath, sheet_name='U1400P1')\n",
    "U1400P2=pd.read_excel(filepath, sheet_name='U1400P2')\n",
    "U1400P3S01=pd.read_excel(filepath, sheet_name='U1400P3S01')\n",
    "U1400P3S02=pd.read_excel(filepath, sheet_name='U1400P3S02')\n",
    "U1400P3S03=pd.read_excel(filepath, sheet_name='U1400P3S03')\n",
    "U1400P3S04=pd.read_excel(filepath, sheet_name='U1400P3S04')\n",
    "U1400P3S05=pd.read_excel(filepath, sheet_name='U1400P3S05')\n",
    "U1400P3S06=pd.read_excel(filepath, sheet_name='U1400P3S06')\n",
    "U1400P3S07=pd.read_excel(filepath, sheet_name='U1400P3S07')\n",
    "U1400P3S08=pd.read_excel(filepath, sheet_name='U1400P3S08')\n",
    "U1400P3S09=pd.read_excel(filepath, sheet_name='U1400P3S09')\n",
    "U1400P3S11=pd.read_excel(filepath, sheet_name='U1400P3S11')\n",
    "U1400P3S12=pd.read_excel(filepath, sheet_name='U1400P3S12')\n",
    "U1400P3S13=pd.read_excel(filepath, sheet_name='U1400P3S13')\n",
    "U1400P3S14=pd.read_excel(filepath, sheet_name='U1400P3S14')\n",
    "U1400P4S01=pd.read_excel(filepath, sheet_name='U1400P4S01')\n",
    "U1400P4S02=pd.read_excel(filepath, sheet_name='U1400P4S02')\n",
    "U1400P4S03=pd.read_excel(filepath, sheet_name='U1400P4S03')\n",
    "U1400P4S04=pd.read_excel(filepath, sheet_name='U1400P4S04')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sheet_name, df in data98.items():\n",
    "    if 'Valre' in df.columns:\n",
    "        df['Valre'] = pd.to_nrmeric(df['Valre'], errors='coerce')\n",
    "\n",
    "    data98[sheet_name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "U1400Data = pd.read_excel(filepath, sheet_name='U1400Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Address</th>\n",
       "      <th>Fasl</th>\n",
       "      <th>weight</th>\n",
       "      <th>khanevartype</th>\n",
       "      <th>Takmil</th>\n",
       "      <th>TakmilDescA</th>\n",
       "      <th>TakmilDescB</th>\n",
       "      <th>TakmilDescC</th>\n",
       "      <th>Jaygozin</th>\n",
       "      <th>JaygozinDescA</th>\n",
       "      <th>JaygozinDescB</th>\n",
       "      <th>JaygozinDescC</th>\n",
       "      <th>BlkAbdJaygozin</th>\n",
       "      <th>RadifJaygozin</th>\n",
       "      <th>IsTel</th>\n",
       "      <th>FVam</th>\n",
       "      <th>province</th>\n",
       "      <th>town</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10001000119</td>\n",
       "      <td>4</td>\n",
       "      <td>1245</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Markazi</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10001000120</td>\n",
       "      <td>4</td>\n",
       "      <td>1245</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Markazi</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10001000126</td>\n",
       "      <td>4</td>\n",
       "      <td>1245</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Markazi</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10001000130</td>\n",
       "      <td>4</td>\n",
       "      <td>1245</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Markazi</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10001000132</td>\n",
       "      <td>4</td>\n",
       "      <td>1245</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Markazi</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Address  Fasl  weight  khanevartype  Takmil  TakmilDescA  TakmilDescB  \\\n",
       "0  10001000119     4    1245             1       2          NaN          1.0   \n",
       "1  10001000120     4    1245             1       2          NaN          1.0   \n",
       "2  10001000126     4    1245             1       1          NaN          NaN   \n",
       "3  10001000130     4    1245             1       1          NaN          NaN   \n",
       "4  10001000132     4    1245             1       1          NaN          NaN   \n",
       "\n",
       "  TakmilDescC  Jaygozin  JaygozinDescA  JaygozinDescB  JaygozinDescC  \\\n",
       "0         NaN       1.0            NaN            NaN            NaN   \n",
       "1         NaN       1.0            NaN            NaN            NaN   \n",
       "2         NaN       NaN            NaN            NaN            NaN   \n",
       "3         NaN       NaN            NaN            NaN            NaN   \n",
       "4         NaN       NaN            NaN            NaN            NaN   \n",
       "\n",
       "   BlkAbdJaygozin  RadifJaygozin  IsTel  FVam province  town  \n",
       "0             2.0           24.0      1     1  Markazi     1  \n",
       "1             2.0           23.0      1     1  Markazi     1  \n",
       "2             NaN            NaN      1     1  Markazi     1  \n",
       "3             NaN            NaN      1     1  Markazi     1  \n",
       "4             NaN            NaN      1     1  Markazi     1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U1400Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Address             0.000000\n",
       "Fasl                0.000000\n",
       "weight              0.000000\n",
       "khanevartype        0.000000\n",
       "Takmil              0.000000\n",
       "TakmilDescA       100.000000\n",
       "TakmilDescB        76.159649\n",
       "TakmilDescC        94.301152\n",
       "Jaygozin           76.159649\n",
       "JaygozinDescA     100.000000\n",
       "JaygozinDescB     100.000000\n",
       "JaygozinDescC     100.000000\n",
       "BlkAbdJaygozin     76.159649\n",
       "RadifJaygozin      76.159649\n",
       "IsTel               0.000000\n",
       "FVam                0.000000\n",
       "province            0.000000\n",
       "town                0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_percentage = U1400Data.isnull().sum() / len(U1400Data) * 100\n",
    "missing_percentage.sort_values(ascending=False)  \n",
    "missing_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "U1400Data_cleaned = U1400Data.dropna(axis=1, thresh=int((1-threshold/100)*len(U1400Data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Address         0.0\n",
       "Fasl            0.0\n",
       "weight          0.0\n",
       "khanevartype    0.0\n",
       "Takmil          0.0\n",
       "IsTel           0.0\n",
       "FVam            0.0\n",
       "province        0.0\n",
       "town            0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_percentage = U1400Data_cleaned.isnull().sum() / len(U1400Data_cleaned) * 100\n",
    "missing_percentage.sort_values(ascending=False)  \n",
    "missing_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Address         0.0\n",
       "Fasl            0.0\n",
       "weight          0.0\n",
       "khanevartype    0.0\n",
       "Takmil          0.0\n",
       "IsTel           0.0\n",
       "FVam            0.0\n",
       "province        0.0\n",
       "town            0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_percentage = U1400Data_cleaned.isnull().sum() / len(U1400Data_cleaned) * 100\n",
    "missing_percentage.sort_values(ascending=False)  \n",
    "missing_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "duplicates = U1400Data_cleaned.duplicated().sum()\n",
    "print(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ستون‌های دسته‌بندی‌شده:\n",
      "Index(['province'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "categorical_columns = U1400Data_cleaned.select_dtypes(include=['object']).columns\n",
    "print(\"ستون‌های دسته‌بندی‌شده:\")\n",
    "print(categorical_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ستون‌های عددی:\n",
      "Index(['Address', 'Fasl', 'weight', 'khanevartype', 'Takmil', 'IsTel', 'FVam',\n",
      "       'town'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nrmerical_columns = U1400Data_cleaned.select_dtypes(include=['number']).columns\n",
    "print(\"ستون‌های عددی:\")\n",
    "print(nrmerical_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "مقادیر منحصر به فرد در ستون province:\n",
      "['Markazi' 'Gilan' 'Mazandaran' 'AzarbaijanSharghi' 'AzarbaijanGharbi'\n",
      " 'Kermanshah' 'Kouzestan' 'Fars' 'Kerman' 'KhorasanRazavi' 'Esfahan'\n",
      " 'SistanBalouchestan' 'Kordestan' 'Hamedan' 'CharmahalBakhtiari'\n",
      " 'Lorestan' 'Ilam' 'KohkilouyeBoyerahamad' 'Boushehr' 'Zanjan' 'Semnan'\n",
      " 'Yazd' 'Hormozgan' 'Tehran' 'Ardebil' 'Qom' 'Qazvin' 'Golestan'\n",
      " 'KhorasanShomali' 'KhorasanJonoubi' 'Alborz']\n"
     ]
    }
   ],
   "source": [
    "unique_provinces = U1400Data_cleaned['province'].unique()\n",
    "print(\"مقادیر منحصر به فرد در ستون province:\")\n",
    "print(unique_provinces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "تعداد هر دسته در ستون province:\n",
      "province\n",
      "Tehran                   1469\n",
      "Golestan                 1057\n",
      "Hormozgan                 868\n",
      "Gilan                     844\n",
      "SistanBalouchestan        740\n",
      "KhorasanRazavi            709\n",
      "KhorasanShomali           705\n",
      "Hamedan                   701\n",
      "KhorasanJonoubi           652\n",
      "CharmahalBakhtiari        651\n",
      "Fars                      648\n",
      "Esfahan                   637\n",
      "Yazd                      617\n",
      "Zanjan                    611\n",
      "Kouzestan                 610\n",
      "AzarbaijanSharghi         604\n",
      "Markazi                   588\n",
      "Kerman                    581\n",
      "Boushehr                  555\n",
      "AzarbaijanGharbi          545\n",
      "KohkilouyeBoyerahamad     535\n",
      "Qom                       529\n",
      "Kermanshah                513\n",
      "Kordestan                 484\n",
      "Semnan                    478\n",
      "Ardebil                   473\n",
      "Ilam                      465\n",
      "Lorestan                  457\n",
      "Mazandaran                453\n",
      "Qazvin                    420\n",
      "Alborz                    419\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# مشاهده تعداد هر دسته‌بندی در ستون province\n",
    "province_cornts = U1400Data_cleaned['province'].value_counts()\n",
    "print(\"تعداد هر دسته در ستون province:\")\n",
    "print(province_cornts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# دیکشنری برای انکود کردن مقادیر province\n",
    "province_encoding = {\n",
    "    'Markazi': '00', 'Gilan': '01', 'Mazandaran': '02', 'AzarbaijanSharghi': '03', \n",
    "    'AzarbaijanGharbi': '04', 'Kermanshah': '05', 'Korzestan': '06', 'Fars': '07', \n",
    "    'Kerman': '08', 'Khorasanrazavi': '09', 'Esfahan': '10', 'SistanBalorchestan': '11', \n",
    "    'Kordestan': '12', 'Hamedan': '13', 'CharmahalBakhtiari': '14', 'Lorestan': '15', \n",
    "    'Ilam': '16', 'KohkiloryeBoyerahamad': '17', 'Borshehr': '18', 'Zanjan': '19', \n",
    "    'Semnan': '20', 'Yazd': '21', 'Hormozgan': '22', 'Tehran': '23', 'Ardebil': '24', \n",
    "    'Qom': '25', 'Qazvin': '26', 'Golestan': '27', 'KhorasanShomali': '28', \n",
    "    'KhorasanJonorbi': '29', 'Alborz': '30'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  province province_encoded\n",
      "0  Markazi               00\n",
      "1  Markazi               00\n",
      "2  Markazi               00\n",
      "3  Markazi               00\n",
      "4  Markazi               00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_1052\\350281337.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  U1400Data_cleaned['province_encoded'] = U1400Data_cleaned['province'].map(province_encoding)\n"
     ]
    }
   ],
   "source": [
    "# انکود کردن ستون province با استفاده از دیکشنری\n",
    "U1400Data_cleaned['province_encoded'] = U1400Data_cleaned['province'].map(province_encoding)\n",
    "\n",
    "# بررسی نتیجه\n",
    "print(U1400Data_cleaned[['province', 'province_encoded']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "مقادیر منحصر به فرد در ستون province_encoded:\n",
      "['00' '01' '02' '03' '04' '05' nan '07' '08' '10' '12' '13' '14' '15' '16'\n",
      " '19' '20' '21' '22' '23' '24' '25' '26' '27' '28' '30']\n"
     ]
    }
   ],
   "source": [
    "unique_provinces = U1400Data_cleaned['province_encoded'].unique()\n",
    "print(\"مقادیر منحصر به فرد در ستون province_encoded:\")\n",
    "print(unique_provinces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "U1400Data_cleaned = U1400Data_cleaned.drop(columns=['province'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "تعداد داده‌های پرت در ستون weight: 727\n",
      "تعداد داده‌های پرت در ستون khanevartype: 16\n",
      "تعداد داده‌های پرت در ستون town: 845\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# بررسی داده‌های پرت بر اساس IQr\n",
    "for column in nrmerical_columns:\n",
    "    Q1 = U1400Data_cleaned[column].quantile(0.25)\n",
    "    Q3 = U1400Data_cleaned[column].quantile(0.75)\n",
    "    IQr = Q3 - Q1\n",
    "    ortliers = ((U1400Data_cleaned[column] < (Q1 - 1.5 * IQr)) | (U1400Data_cleaned[column] > (Q3 + 1.5 * IQr))).sum()\n",
    "    if ortliers > 0:\n",
    "        print(f\"تعداد داده‌های پرت در ستون {column}: {ortliers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# محاسبه IQr و اعمال بازه‌بندی (Capping)\n",
    "for column in ['weight']:\n",
    "    Q1 = U1400Data_cleaned[column].quantile(0.25)\n",
    "    Q3 = U1400Data_cleaned[column].quantile(0.75)\n",
    "    IQr = Q3 - Q1\n",
    "    \n",
    "    lower_bornd = Q1 - 1.5 * IQr\n",
    "    rpper_bornd = Q3 + 1.5 * IQr\n",
    "    \n",
    "    # اعمال بازه‌بندی\n",
    "    U1400Data_cleaned[column] = np.where(U1400Data_cleaned[column] < lower_bornd, lower_bornd, U1400Data_cleaned[column])\n",
    "    U1400Data_cleaned[column] = np.where(U1400Data_cleaned[column] > rpper_bornd, rpper_bornd, U1400Data_cleaned[column])\n",
    "    \n",
    "    # اعمال تبدیل لگاریتمی برای مقادیر مثبت\n",
    "    U1400Data_cleaned[column] = np.log1p(U1400Data_cleaned[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# جایگزینی مقادیر پرت با Mode (پرتکرارترین مقدار)\n",
    "def replace_ortliers_with_mode(column, threshold=0.01):\n",
    "    mode = U1400Data_cleaned[column].mode()[0]\n",
    "    \n",
    "    # شناسایی دسته‌های نادر (کمتر از آستانه threshold)\n",
    "    value_counts = U1400Data_cleaned[column].value_counts(normalize=True)\n",
    "    rare_valres = value_counts[value_counts < threshold].index\n",
    "    \n",
    "    # جایگزینی مقادیر نادر با Mode\n",
    "    U1400Data_cleaned[column] = U1400Data_cleaned[column].apply(lambda x: mode if x in rare_valres else x)\n",
    "\n",
    "# اعمال جایگزینی با Mode برای ستون‌های دسته‌ای\n",
    "for column in ['khanevartype', 'Takmil', 'town']:\n",
    "    replace_ortliers_with_mode(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# بررسی و اصلاح اوت\\u200cلیرهای نادرست در ستون\\u200cهای MahMorajeh و Fasl\\n# برای ماه محدوده باید بین 1 تا 12 باشد و برای فصل باید بین 1 تا 4 باشد\\ndef fix_invalid_valres(column, valid_range):\\n    U1400Data_cleaned[column] = np.where(~U1400Data_cleaned[column].isin(valid_range), np.nan, U1400Data_cleaned[column])\\n\\n# اعمال اصلاح برای ستون\\u200cهای MahMorajeh و Fasl\\nfix_invalid_valres('MahMorajeh', range(1, 13))  # ماه\\u200cها باید بین 1 تا 12 باشند\\nfix_invalid_valres('Fasl', range(1, 5))  # فصل\\u200cها باید بین 1 تا 4 باشند\\n\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# بررسی و اصلاح اوت‌لیرهای نادرست در ستون‌های MahMorajeh و Fasl\n",
    "# برای ماه محدوده باید بین 1 تا 12 باشد و برای فصل باید بین 1 تا 4 باشد\n",
    "def fix_invalid_valres(column, valid_range):\n",
    "    U1400Data_cleaned[column] = np.where(~U1400Data_cleaned[column].isin(valid_range), np.nan, U1400Data_cleaned[column])\n",
    "\n",
    "# اعمال اصلاح برای ستون‌های MahMorajeh و Fasl\n",
    "fix_invalid_valres('MahMorajeh', range(1, 13))  # ماه‌ها باید بین 1 تا 12 باشند\n",
    "fix_invalid_valres('Fasl', range(1, 5))  # فصل‌ها باید بین 1 تا 4 باشند\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  2  3  4  5  6  7  9 10 11 12 13  8 14 16]\n"
     ]
    }
   ],
   "source": [
    "unique_valres = U1400Data_cleaned['town'].unique()\n",
    "print(unique_valres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1]\n"
     ]
    }
   ],
   "source": [
    "unique_valres = U1400Data_cleaned['Takmil'].unique()\n",
    "print(unique_valres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19618 entries, 0 to 19617\n",
      "Data columns (total 9 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   Address           19618 non-null  int64  \n",
      " 1   Fasl              19618 non-null  int64  \n",
      " 2   weight            19618 non-null  float64\n",
      " 3   khanevartype      19618 non-null  int64  \n",
      " 4   Takmil            19618 non-null  int64  \n",
      " 5   IsTel             19618 non-null  int64  \n",
      " 6   FVam              19618 non-null  int64  \n",
      " 7   town              19618 non-null  int64  \n",
      " 8   province_encoded  15817 non-null  object \n",
      "dtypes: float64(1), int64(7), object(1)\n",
      "memory usage: 1.3+ MB\n"
     ]
    }
   ],
   "source": [
    "U1400Data_cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Address', 'member', 'relation', 'gender', 'age', 'literacy',\n",
       "       'studying', 'degree', 'occupationalst', 'maritalst'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U1400P1.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Address            0.000000\n",
       "member             0.000000\n",
       "relation           0.000000\n",
       "gender             0.000000\n",
       "age                0.000000\n",
       "literacy           7.441388\n",
       "studying          17.462315\n",
       "degree            17.463832\n",
       "occupationalst    13.969852\n",
       "maritalst         13.966819\n",
       "dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_percentage = U1400P1.isnull().sum() / len(U1400P1) * 100\n",
    "missing_percentage.sort_values(ascending=False)  \n",
    "missing_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "تعداد ردیف‌های تکراری: 0\n"
     ]
    }
   ],
   "source": [
    "# بررسی وجود داده‌های تکراری\n",
    "drplicate_rows = U1400P1.duplicated().sum()\n",
    "print(f\"تعداد ردیف‌های تکراری: {drplicate_rows}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ستون‌های دسته‌بندی‌شده:\n",
      "Index(['relation', 'gender', 'literacy', 'studying', 'degree',\n",
      "       'occupationalst', 'maritalst'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# بررسی ستون‌های دسته‌بندی‌شده (غیر عددی)\n",
    "categorical_columns = U1400P1.select_dtypes(include=['object']).columns\n",
    "print(\"ستون‌های دسته‌بندی‌شده:\")\n",
    "print(categorical_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "مقادیر یونیک در ستون 'relation':\n",
      "['Head' 'Spouse' 'Child' 'Parent' 'SonDaughter_inLaw' 'GrandSonDaughter'\n",
      " 'Sibling' 'NonRelative' 'OtherRelative']\n",
      "--------------------------------------------------\n",
      "مقادیر یونیک در ستون 'gender':\n",
      "['Male' 'Female']\n",
      "--------------------------------------------------\n",
      "مقادیر یونیک در ستون 'literacy':\n",
      "['literate' nan 'illiterate']\n",
      "--------------------------------------------------\n",
      "مقادیر یونیک در ستون 'studying':\n",
      "['No' 'Yes' nan]\n",
      "--------------------------------------------------\n",
      "مقادیر یونیک در ستون 'degree':\n",
      "['Secondary' 'Diploma' 'Elemantry' nan 'Bachelor' 'Other' 'Master'\n",
      " 'HighSchool' 'College' 'PhD']\n",
      "--------------------------------------------------\n",
      "مقادیر یونیک در ستون 'occupationalst':\n",
      "['employed' 'Housewife' nan 'Student' 'IncomeWOJob' 'unemployed' 'Other']\n",
      "--------------------------------------------------\n",
      "مقادیر یونیک در ستون 'maritalst':\n",
      "['Married' nan 'Single' 'Widowed' 'Divorced']\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# لیست ستون‌های دسته‌بندی‌شده\n",
    "categorical_columns = ['relation', 'gender', 'literacy', 'studying', 'degree', 'occupationalst', 'maritalst']\n",
    "\n",
    "# حلقه برای مشاهده داده‌های یونیک در هر ستون\n",
    "for column in categorical_columns:\n",
    "    unique_valres = U1400P1[column].unique()\n",
    "    print(f\"مقادیر یونیک در ستون '{column}':\")\n",
    "    print(unique_valres)\n",
    "    print(\"-\" * 50)  # جداکننده برای خوانایی بیشتر\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# دیکشنری‌های انکودینگ\n",
    "relation_encoding = {\n",
    "    'Head': '1', 'Sporse': '2', 'Child': '3', 'SonDarghter_inLaw': '4', \n",
    "    'GrandSonDarghter': '5', 'Parent': '6', 'Sibling': '7', 'Otherrelative': '8', \n",
    "    'Nonrelative': '9'\n",
    "}\n",
    "\n",
    "gender_encoding = {'Male': '1', 'Female': '2'}\n",
    "\n",
    "literacy_encoding = {'literate': '1', 'illiterate': '2'}\n",
    "\n",
    "yesno_encoding = {'Yes': '1', 'No': '2'}\n",
    "\n",
    "edrcation_encoding = {\n",
    "    'Elemantry': '1', 'Secondary': '2', 'HighSchool': '3', 'Diploma': '4', \n",
    "    'College': '5', 'Bachelor': '6', 'Master': '7', 'PhD': '8', 'Other': '9'\n",
    "}\n",
    "\n",
    "occrpation_encoding = {\n",
    "    'employed': '1', 'rnemployed': '2', 'IncomeWOJob': '3', 'Strdent': '4', \n",
    "    'Horsewife': '5', 'Other': '6'\n",
    "}\n",
    "\n",
    "marital_encoding = {'Married': '1', 'Widowed': '2', 'Divorced': '3', 'Single': '4'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "U1400P1['relation_encoded'] = U1400P1['relation'].map(relation_encoding)\n",
    "U1400P1['gender_encoded'] = U1400P1['gender'].map(gender_encoding)\n",
    "U1400P1['literacy_encoded'] = U1400P1['literacy'].map(literacy_encoding)\n",
    "U1400P1['studying_encoded'] = U1400P1['studying'].map(yesno_encoding)\n",
    "U1400P1['degree_encoded'] = U1400P1['degree'].map(edrcation_encoding)\n",
    "U1400P1['occupationalst_encoded'] = U1400P1['occupationalst'].map(occrpation_encoding)\n",
    "U1400P1['maritalst_encoded'] = U1400P1['maritalst'].map(marital_encoding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "U1400P1 = U1400P1.drop(columns=['relation', 'gender', 'literacy', 'studying', 'degree', 'occupationalst', 'maritalst'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Address</th>\n",
       "      <th>member</th>\n",
       "      <th>age</th>\n",
       "      <th>relation_encoded</th>\n",
       "      <th>gender_encoded</th>\n",
       "      <th>literacy_encoded</th>\n",
       "      <th>studying_encoded</th>\n",
       "      <th>degree_encoded</th>\n",
       "      <th>occupationalst_encoded</th>\n",
       "      <th>maritalst_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10011009725</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10011009725</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10011009720</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10011009720</td>\n",
       "      <td>2</td>\n",
       "      <td>31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10011009720</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Address  member  age relation_encoded gender_encoded literacy_encoded  \\\n",
       "0  10011009725       1   28                1              1                1   \n",
       "1  10011009725       2   18              NaN              2                1   \n",
       "2  10011009720       1   38                1              1                1   \n",
       "3  10011009720       2   31              NaN              2                1   \n",
       "4  10011009720       3    7                3              1                1   \n",
       "\n",
       "  studying_encoded degree_encoded occupationalst_encoded maritalst_encoded  \n",
       "0                2              2                      1                 1  \n",
       "1                2              4                    NaN                 1  \n",
       "2                2              2                      1                 1  \n",
       "3                2              1                    NaN                 1  \n",
       "4                1              1                    NaN               NaN  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U1400P1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "نوع داده‌های هر ستون:\n",
      "Address                    int64\n",
      "member                     int64\n",
      "age                        int64\n",
      "relation_encoded          object\n",
      "gender_encoded            object\n",
      "literacy_encoded          object\n",
      "studying_encoded          object\n",
      "degree_encoded            object\n",
      "occupationalst_encoded    object\n",
      "maritalst_encoded         object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# نمایش نوع داده‌ها برای هر ستون\n",
    "print(\"نوع داده‌های هر ستون:\")\n",
    "print(U1400P1.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "مقادیر یونیک در ستون 'relation_encoded': ['1' nan '3' '6' '7']\n",
      "مقادیر یونیک در ستون 'gender_encoded': ['1' '2']\n",
      "مقادیر یونیک در ستون 'literacy_encoded': ['1' nan '2']\n",
      "مقادیر یونیک در ستون 'studying_encoded': ['2' '1' nan]\n",
      "مقادیر یونیک در ستون 'degree_encoded': ['2' '4' '1' nan '6' '9' '7' '3' '5' '8']\n",
      "مقادیر یونیک در ستون 'occupationalst_encoded': ['1' nan '3' '6']\n",
      "مقادیر یونیک در ستون 'maritalst_encoded': ['1' nan '4' '2' '3']\n"
     ]
    }
   ],
   "source": [
    "# بررسی مقادیر یونیک در هر ستون انکود شده\n",
    "print(\"مقادیر یونیک در ستون 'relation_encoded':\", U1400P1['relation_encoded'].unique())\n",
    "print(\"مقادیر یونیک در ستون 'gender_encoded':\", U1400P1['gender_encoded'].unique())\n",
    "print(\"مقادیر یونیک در ستون 'literacy_encoded':\", U1400P1['literacy_encoded'].unique())\n",
    "print(\"مقادیر یونیک در ستون 'studying_encoded':\", U1400P1['studying_encoded'].unique())\n",
    "print(\"مقادیر یونیک در ستون 'degree_encoded':\", U1400P1['degree_encoded'].unique())\n",
    "print(\"مقادیر یونیک در ستون 'occupationalst_encoded':\", U1400P1['occupationalst_encoded'].unique())\n",
    "print(\"مقادیر یونیک در ستون 'maritalst_encoded':\", U1400P1['maritalst_encoded'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded = U1400P1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# بررسی ستون‌ها و جدا کردن داده‌های مفقود و غیر مفقود\n",
    "df_not_null = df_encoded[df_encoded['studying_encoded'].notnull()]\n",
    "df_null = df_encoded[df_encoded['studying_encoded'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_not_null.drop(['studying_encoded'], axis=1)\n",
    "y = df_not_null['studying_encoded']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9683079184273379\n",
      "recall: 0.9328214971209213\n",
      "F1 Score: 0.9441476444876153\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "y_test_nrmeric = y_test.astype(int)\n",
    "y_pred_nrmeric = y_pred.astype(int)\n",
    "\n",
    "# محاسبه دقت (accuracy)\n",
    "accuracy = accuracy_score(y_test_nrmeric, y_pred_nrmeric)\n",
    "print(f'accuracy: {accuracy}')\n",
    "\n",
    "# محاسبه recall و F1 Score\n",
    "recall = recall_score(y_test_nrmeric, y_pred_nrmeric)\n",
    "f1 = f1_score(y_test_nrmeric, y_pred_nrmeric)\n",
    "\n",
    "print(f'recall: {recall}')\n",
    "print(f'F1 Score: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Address  member  age relation_encoded gender_encoded literacy_encoded  \\\n",
      "0  10011009725       1   28                1              1                1   \n",
      "1  10011009725       2   18              NaN              2                1   \n",
      "2  10011009720       1   38                1              1                1   \n",
      "3  10011009720       2   31              NaN              2                1   \n",
      "4  10011009720       3    7                3              1                1   \n",
      "\n",
      "  studying_encoded degree_encoded occupationalst_encoded maritalst_encoded  \n",
      "0                2              2                      1                 1  \n",
      "1                2              4                    NaN                 1  \n",
      "2                2              2                      1                 1  \n",
      "3                2              1                    NaN                 1  \n",
      "4                1              1                    NaN               NaN  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_null = df_null.drop(['studying_encoded'], axis=1)\n",
    "\n",
    "\n",
    "df_null.loc[:, 'studying_encoded'] = model.predict(X_null)\n",
    "\n",
    "\n",
    "U1400P1_filled = pd.concat([df_not_null, df_null])\n",
    "\n",
    "\n",
    "U1400P1_filled.sort_index(inplace=True)\n",
    "\n",
    "\n",
    "print(U1400P1_filled.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Address                    0.000000\n",
       "member                     0.000000\n",
       "age                        0.000000\n",
       "relation_encoded          25.856055\n",
       "gender_encoded             0.000000\n",
       "literacy_encoded           7.441388\n",
       "studying_encoded           0.000000\n",
       "degree_encoded            17.463832\n",
       "occupationalst_encoded    60.415213\n",
       "maritalst_encoded         13.966819\n",
       "dtype: float64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_percentage = U1400P1_filled.isnull().sum() / len(U1400P1) * 100\n",
    "missing_percentage.sort_values(ascending=False)  \n",
    "missing_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded2 = U1400P1_filled.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9683079184273379\n",
      "       Address  member  age relation_encoded gender_encoded literacy_encoded  \\\n",
      "0  10011009725       1   28                1              1                1   \n",
      "1  10011009725       2   18              NaN              2                1   \n",
      "2  10011009720       1   38                1              1                1   \n",
      "3  10011009720       2   31              NaN              2                1   \n",
      "4  10011009720       3    7                3              1                1   \n",
      "\n",
      "  studying_encoded degree_encoded occupationalst_encoded maritalst_encoded  \n",
      "0                2              2                      1                 1  \n",
      "1                2              4                    NaN                 1  \n",
      "2                2              2                      1                 1  \n",
      "3                2              1                    NaN                 1  \n",
      "4                1              1                    NaN               NaN  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_not_null = df_encoded2[df_encoded2['literacy_encoded'].notnull()]\n",
    "df_null = df_encoded2[df_encoded2['literacy_encoded'].isnull()]\n",
    "\n",
    "\n",
    "X = df_not_null.drop(['literacy_encoded'], axis=1)\n",
    "y = df_not_null['literacy_encoded']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(y_test_nrmeric, y_pred_nrmeric)\n",
    "print(f'accuracy: {accuracy}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_null = df_null.drop(['literacy_encoded'], axis=1)\n",
    "\n",
    "\n",
    "df_null.loc[:, 'literacy_encoded'] = model.predict(X_null)\n",
    "\n",
    "\n",
    "U1400P1_filled = pd.concat([df_not_null, df_null])\n",
    "\n",
    "\n",
    "U1400P1_filled.sort_index(inplace=True)\n",
    "\n",
    "\n",
    "print(U1400P1_filled.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Address                    0.000000\n",
       "member                     0.000000\n",
       "age                        0.000000\n",
       "relation_encoded          25.856055\n",
       "gender_encoded             0.000000\n",
       "literacy_encoded           0.000000\n",
       "studying_encoded           0.000000\n",
       "degree_encoded            17.463832\n",
       "occupationalst_encoded    60.415213\n",
       "maritalst_encoded         13.966819\n",
       "dtype: float64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_percentage = U1400P1_filled.isnull().sum() / len(U1400P1) * 100\n",
    "missing_percentage.sort_values(ascending=False)  \n",
    "missing_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded3 = U1400P1_filled.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for degree_encoded: 0.44855778063567886\n",
      "       Address  member  age relation_encoded gender_encoded literacy_encoded  \\\n",
      "0  10011009725       1   28                1              1                1   \n",
      "1  10011009725       2   18              NaN              2                1   \n",
      "2  10011009720       1   38                1              1                1   \n",
      "3  10011009720       2   31              NaN              2                1   \n",
      "4  10011009720       3    7                3              1                1   \n",
      "\n",
      "  studying_encoded degree_encoded occupationalst_encoded maritalst_encoded  \n",
      "0                2              2                      1                 1  \n",
      "1                2              4                    NaN                 1  \n",
      "2                2              2                      1                 1  \n",
      "3                2              1                    NaN                 1  \n",
      "4                1              1                    NaN               NaN  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_not_null = df_encoded3[df_encoded3['degree_encoded'].notnull()]\n",
    "df_null = df_encoded3[df_encoded3['degree_encoded'].isnull()]\n",
    "\n",
    "\n",
    "X = df_not_null.drop(['degree_encoded'], axis=1)\n",
    "y = df_not_null['degree_encoded']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "print(f'accuracy for degree_encoded: {accuracy_score(y_test, y_pred)}')\n",
    "\n",
    "\n",
    "X_null = df_null.drop(['degree_encoded'], axis=1)\n",
    "\n",
    "\n",
    "df_null.loc[:, 'degree_encoded'] = model.predict(X_null)\n",
    "\n",
    "\n",
    "U1400P1_filled = pd.concat([df_not_null, df_null])\n",
    "\n",
    "\n",
    "U1400P1_filled.sort_index(inplace=True)\n",
    "\n",
    "\n",
    "print(U1400P1_filled.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Address                    0.000000\n",
       "member                     0.000000\n",
       "age                        0.000000\n",
       "relation_encoded          25.856055\n",
       "gender_encoded             0.000000\n",
       "literacy_encoded           0.000000\n",
       "studying_encoded           0.000000\n",
       "degree_encoded             0.000000\n",
       "occupationalst_encoded    60.415213\n",
       "maritalst_encoded         13.966819\n",
       "dtype: float64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_percentage = U1400P1_filled.isnull().sum() / len(U1400P1) * 100\n",
    "missing_percentage.sort_values(ascending=False)  \n",
    "missing_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded4 = U1400P1_filled.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for occupationalst_encoded : 0.8142118368128711\n",
      "       Address  member  age relation_encoded gender_encoded literacy_encoded  \\\n",
      "0  10011009725       1   28                1              1                1   \n",
      "1  10011009725       2   18              NaN              2                1   \n",
      "2  10011009720       1   38                1              1                1   \n",
      "3  10011009720       2   31              NaN              2                1   \n",
      "4  10011009720       3    7                3              1                1   \n",
      "\n",
      "  studying_encoded degree_encoded occupationalst_encoded maritalst_encoded  \n",
      "0                2              2                      1                 1  \n",
      "1                2              4                      1                 1  \n",
      "2                2              2                      1                 1  \n",
      "3                2              1                      1                 1  \n",
      "4                1              1                      1               NaN  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_not_null = df_encoded4[df_encoded4['occupationalst_encoded'].notnull()]\n",
    "df_null = df_encoded4[df_encoded4['occupationalst_encoded'].isnull()]\n",
    "\n",
    "\n",
    "X = df_not_null.drop(['occupationalst_encoded'], axis=1)\n",
    "y = df_not_null['occupationalst_encoded']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "print(f'accuracy for occupationalst_encoded : {accuracy_score(y_test, y_pred)}')\n",
    "\n",
    "\n",
    "X_null = df_null.drop(['occupationalst_encoded'], axis=1)\n",
    "\n",
    "\n",
    "df_null.loc[:, 'occupationalst_encoded'] = model.predict(X_null)\n",
    "\n",
    "\n",
    "U1400P1_filled = pd.concat([df_not_null, df_null])\n",
    "\n",
    "\n",
    "U1400P1_filled.sort_index(inplace=True)\n",
    "\n",
    "\n",
    "print(U1400P1_filled.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Address                    0.000000\n",
       "member                     0.000000\n",
       "age                        0.000000\n",
       "relation_encoded          25.856055\n",
       "gender_encoded             0.000000\n",
       "literacy_encoded           0.000000\n",
       "studying_encoded           0.000000\n",
       "degree_encoded             0.000000\n",
       "occupationalst_encoded     0.000000\n",
       "maritalst_encoded         13.966819\n",
       "dtype: float64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_percentage = U1400P1_filled.isnull().sum() / len(U1400P1) * 100\n",
    "missing_percentage.sort_values(ascending=False)  \n",
    "missing_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded5 = U1400P1_filled.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for maritalst_encoded : 0.9434211685908169\n",
      "       Address  member  age relation_encoded gender_encoded literacy_encoded  \\\n",
      "0  10011009725       1   28                1              1                1   \n",
      "1  10011009725       2   18              NaN              2                1   \n",
      "2  10011009720       1   38                1              1                1   \n",
      "3  10011009720       2   31              NaN              2                1   \n",
      "4  10011009720       3    7                3              1                1   \n",
      "\n",
      "  studying_encoded degree_encoded occupationalst_encoded maritalst_encoded  \n",
      "0                2              2                      1                 1  \n",
      "1                2              4                      1                 1  \n",
      "2                2              2                      1                 1  \n",
      "3                2              1                      1                 1  \n",
      "4                1              1                      1                 4  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_not_null = df_encoded5[df_encoded5['maritalst_encoded'].notnull()]\n",
    "df_null = df_encoded5[df_encoded5['maritalst_encoded'].isnull()]\n",
    "\n",
    "\n",
    "X = df_not_null.drop(['maritalst_encoded'], axis=1)\n",
    "y = df_not_null['maritalst_encoded']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "print(f'Accuracy for maritalst_encoded : {accuracy_score(y_test, y_pred)}')\n",
    "\n",
    "\n",
    "X_null = df_null.drop(['maritalst_encoded'], axis=1)\n",
    "\n",
    "\n",
    "df_null.loc[:, 'maritalst_encoded'] = model.predict(X_null)\n",
    "\n",
    "\n",
    "U1400P1_filled = pd.concat([df_not_null, df_null])\n",
    "\n",
    "\n",
    "U1400P1_filled.sort_index(inplace=True)\n",
    "\n",
    "\n",
    "print(U1400P1_filled.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Address       0.000000\n",
       "code          0.000000\n",
       "purchased     0.000000\n",
       "gram         73.106829\n",
       "kilogram     24.583824\n",
       "price         8.997146\n",
       "value         0.000162\n",
       "dtype: float64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_percentage = U1400P3S01.isnull().sum() / len(U1400P3S01) * 100\n",
    "missing_percentage.sort_values(ascending=False)  \n",
    "missing_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "U1400P3S01_cleaned = U1400P3S01.dropna(subset=['price', 'value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_1052\\1356400674.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  U1400P3S01_cleaned['calculated_amount'] = U1400P3S01_cleaned['value'] / U1400P3S01_cleaned['price']\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_1052\\1356400674.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  U1400P3S01_cleaned['calculated_kg'] = U1400P3S01_cleaned['calculated_amount'].apply(np.floor)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_1052\\1356400674.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  U1400P3S01_cleaned['calculated_gram'] = (U1400P3S01_cleaned['calculated_amount'] % 1) * 1000\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_1052\\1356400674.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  U1400P3S01_cleaned['kilogram'] = U1400P3S01_cleaned['kilogram'].fillna(U1400P3S01_cleaned['calculated_kg'])\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_1052\\1356400674.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  U1400P3S01_cleaned['gram'] = U1400P3S01_cleaned['gram'].fillna(U1400P3S01_cleaned['calculated_gram'])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "U1400P3S01_cleaned['calculated_amount'] = U1400P3S01_cleaned['value'] / U1400P3S01_cleaned['price']\n",
    "\n",
    "\n",
    "U1400P3S01_cleaned['calculated_kg'] = U1400P3S01_cleaned['calculated_amount'].apply(np.floor)\n",
    "U1400P3S01_cleaned['calculated_gram'] = (U1400P3S01_cleaned['calculated_amount'] % 1) * 1000\n",
    "\n",
    "\n",
    "U1400P3S01_cleaned['kilogram'] = U1400P3S01_cleaned['kilogram'].fillna(U1400P3S01_cleaned['calculated_kg'])\n",
    "U1400P3S01_cleaned['gram'] = U1400P3S01_cleaned['gram'].fillna(U1400P3S01_cleaned['calculated_gram'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Address              0.0\n",
       "code                 0.0\n",
       "purchased            0.0\n",
       "gram                 0.0\n",
       "kilogram             0.0\n",
       "price                0.0\n",
       "value                0.0\n",
       "calculated_amount    0.0\n",
       "calculated_kg        0.0\n",
       "calculated_gram      0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_percentage = U1400P3S01_cleaned.isnull().sum() / len(U1400P3S01_cleaned) * 100\n",
    "missing_percentage.sort_values(ascending=False)  \n",
    "missing_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ستون‌های عددی:\n",
      "Index(['Address', 'code', 'purchased', 'kilogram', 'price', 'value',\n",
      "       'calculated_amount', 'calculated_kg', 'calculated_gram'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "numerical_columns = U1400P3S01_cleaned.select_dtypes(include=['number']).columns\n",
    "print(\"ستون‌های عددی:\")\n",
    "print(numerical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Address        0.00000\n",
       "code           0.00000\n",
       "purchased      0.00000\n",
       "gram         100.00000\n",
       "kilogram      17.24353\n",
       "price         17.24353\n",
       "value          0.00000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_percentage = U1400P3S02.isnull().sum() / len(U1400P3S02) * 100\n",
    "missing_percentage.sort_values(ascending=False)  \n",
    "missing_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "U1400P3S02_cleaned = U1400P3S02.dropna(subset=['price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_1052\\1800257632.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  U1400P3S02_cleaned['calculated_amount'] = U1400P3S02_cleaned['value'] / U1400P3S02_cleaned['price']\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_1052\\1800257632.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  U1400P3S02_cleaned['calculated_kg'] = U1400P3S02_cleaned['calculated_amount'].apply(np.floor)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_1052\\1800257632.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  U1400P3S02_cleaned['calculated_gram'] = (U1400P3S02_cleaned['calculated_amount'] % 1) * 1000\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_1052\\1800257632.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  U1400P3S02_cleaned['kilogram'] = U1400P3S02_cleaned['kilogram'].fillna(U1400P3S02_cleaned['calculated_kg'])\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_1052\\1800257632.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  U1400P3S02_cleaned['gram'] = U1400P3S02_cleaned['gram'].fillna(U1400P3S02_cleaned['calculated_gram'])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "U1400P3S02_cleaned['calculated_amount'] = U1400P3S02_cleaned['value'] / U1400P3S02_cleaned['price']\n",
    "\n",
    "\n",
    "U1400P3S02_cleaned['calculated_kg'] = U1400P3S02_cleaned['calculated_amount'].apply(np.floor)\n",
    "U1400P3S02_cleaned['calculated_gram'] = (U1400P3S02_cleaned['calculated_amount'] % 1) * 1000\n",
    "\n",
    "\n",
    "U1400P3S02_cleaned['kilogram'] = U1400P3S02_cleaned['kilogram'].fillna(U1400P3S02_cleaned['calculated_kg'])\n",
    "U1400P3S02_cleaned['gram'] = U1400P3S02_cleaned['gram'].fillna(U1400P3S02_cleaned['calculated_gram'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Address              0.0\n",
       "code                 0.0\n",
       "purchased            0.0\n",
       "gram                 0.0\n",
       "kilogram             0.0\n",
       "price                0.0\n",
       "value                0.0\n",
       "calculated_amount    0.0\n",
       "calculated_kg        0.0\n",
       "calculated_gram      0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_percentage = U1400P3S01_cleaned.isnull().sum() / len(U1400P3S01_cleaned) * 100\n",
    "missing_percentage.sort_values(ascending=False)  \n",
    "missing_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ستون‌های عددی:\n",
      "Index(['Address', 'code', 'purchased', 'kilogram', 'price', 'value',\n",
      "       'calculated_amount', 'calculated_kg', 'calculated_gram'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "numerical_columns = U1400P3S01_cleaned.select_dtypes(include=['number']).columns\n",
    "print(\"ستون‌های عددی:\")\n",
    "print(numerical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Address        0.00000\n",
       "code           0.00000\n",
       "purchased      0.00000\n",
       "gram         100.00000\n",
       "kilogram      17.24353\n",
       "price         17.24353\n",
       "value          0.00000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_percentage = U1400P3S02.isnull().sum() / len(U1400P3S02) * 100\n",
    "missing_percentage.sort_values(ascending=False)  \n",
    "missing_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "U1400P3S02_cleaned = U1400P3S02.dropna(subset=['price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_1052\\1800257632.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  U1400P3S02_cleaned['calculated_amount'] = U1400P3S02_cleaned['value'] / U1400P3S02_cleaned['price']\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_1052\\1800257632.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  U1400P3S02_cleaned['calculated_kg'] = U1400P3S02_cleaned['calculated_amount'].apply(np.floor)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_1052\\1800257632.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  U1400P3S02_cleaned['calculated_gram'] = (U1400P3S02_cleaned['calculated_amount'] % 1) * 1000\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_1052\\1800257632.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  U1400P3S02_cleaned['kilogram'] = U1400P3S02_cleaned['kilogram'].fillna(U1400P3S02_cleaned['calculated_kg'])\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_1052\\1800257632.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  U1400P3S02_cleaned['gram'] = U1400P3S02_cleaned['gram'].fillna(U1400P3S02_cleaned['calculated_gram'])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "U1400P3S02_cleaned['calculated_amount'] = U1400P3S02_cleaned['value'] / U1400P3S02_cleaned['price']\n",
    "\n",
    "\n",
    "U1400P3S02_cleaned['calculated_kg'] = U1400P3S02_cleaned['calculated_amount'].apply(np.floor)\n",
    "U1400P3S02_cleaned['calculated_gram'] = (U1400P3S02_cleaned['calculated_amount'] % 1) * 1000\n",
    "\n",
    "\n",
    "U1400P3S02_cleaned['kilogram'] = U1400P3S02_cleaned['kilogram'].fillna(U1400P3S02_cleaned['calculated_kg'])\n",
    "U1400P3S02_cleaned['gram'] = U1400P3S02_cleaned['gram'].fillna(U1400P3S02_cleaned['calculated_gram'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Address              0.0\n",
       "code                 0.0\n",
       "purchased            0.0\n",
       "gram                 0.0\n",
       "kilogram             0.0\n",
       "price                0.0\n",
       "value                0.0\n",
       "calculated_amount    0.0\n",
       "calculated_kg        0.0\n",
       "calculated_gram      0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_percentage = U1400P3S02_cleaned.isnull().sum() / len(U1400P3S02_cleaned) * 100\n",
    "missing_percentage.sort_values(ascending=False)  \n",
    "missing_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Address      0.0\n",
       "code         0.0\n",
       "purchased    0.0\n",
       "value        0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_percentage = U1400P3S03.isnull().sum() / len(U1400P3S03) * 100\n",
    "missing_percentage.sort_values(ascending=False)  \n",
    "missing_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ستون‌های دسته‌بندی‌شده:\n",
      "Index([], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "categorical_columns = U1400P3S03.select_dtypes(include=['object']).columns\n",
    "print(\"ستون‌های دسته‌بندی‌شده:\")\n",
    "print(categorical_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "duplicates = U1400P3S03.duplicated().sum()\n",
    "print(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "categorical_columns = U1400P3S03.select_dtypes(include=['object']).columns\n",
    "print(categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ستون‌های عددی:\n",
      "Index(['Address', 'code', 'purchased', 'value'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "numerical_columns = U1400P3S03.select_dtypes(include=['number']).columns\n",
    "print(\"ستون‌های عددی:\")\n",
    "print(numerical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "تعداد داده‌های پرت در ستون code: 4438\n",
      "تعداد داده‌های پرت در ستون purchased: 171\n",
      "تعداد داده‌های پرت در ستون value: 1531\n"
     ]
    }
   ],
   "source": [
    "# بررسی داده‌های پرت بر اساس IQR\n",
    "for column in numerical_columns:\n",
    "    Q1 = U1400P3S03[column].quantile(0.25)\n",
    "    Q3 = U1400P3S03[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    outliers = ((U1400P3S03[column] < (Q1 - 1.5 * IQR)) | (U1400P3S03[column] > (Q3 + 1.5 * IQR))).sum()\n",
    "    if outliers > 0:\n",
    "        print(f\"تعداد داده‌های پرت در ستون {column}: {outliers}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Address       0.000000\n",
       "code          0.000000\n",
       "mortgage     96.203047\n",
       "purchased    16.233283\n",
       "value         0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_percentage = U1400P3S04.isnull().sum() / len(U1400P3S04) * 100\n",
    "missing_percentage.sort_values(ascending=False)  \n",
    "missing_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "U1400P3S04_cleaned = U1400P3S04.dropna(subset=['mortgage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Address      0.0\n",
       "code         0.0\n",
       "mortgage     0.0\n",
       "purchased    0.0\n",
       "value        0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_percentage = U1400P3S04_cleaned.isnull().sum() / len(U1400P3S04_cleaned) * 100\n",
    "missing_percentage.sort_values(ascending=False)  \n",
    "missing_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "duplicates = U1400P3S04_cleaned.duplicated().sum()\n",
    "print(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ستون‌های دسته‌بندی‌شده:\n",
      "Index(['purchased'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "categorical_columns = U1400P3S04_cleaned.select_dtypes(include=['object']).columns\n",
    "print(\"ستون‌های دسته‌بندی‌شده:\")\n",
    "print(categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ستون‌های عددی:\n",
      "Index(['Address', 'code', 'mortgage', 'value'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "numerical_columns = U1400P3S04_cleaned.select_dtypes(include=['number']).columns\n",
    "print(\"ستون‌های عددی:\")\n",
    "print(numerical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "تعداد داده‌های پرت در ستون mortgage: 172\n",
      "تعداد داده‌های پرت در ستون value: 184\n"
     ]
    }
   ],
   "source": [
    "# بررسی داده‌های پرت بر اساس IQR\n",
    "for column in numerical_columns:\n",
    "    Q1 = U1400P3S04_cleaned[column].quantile(0.25)\n",
    "    Q3 = U1400P3S04_cleaned[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    outliers = ((U1400P3S04_cleaned[column] < (Q1 - 1.5 * IQR)) | (U1400P3S04_cleaned[column] > (Q3 + 1.5 * IQR))).sum()\n",
    "    if outliers > 0:\n",
    "        print(f\"تعداد داده‌های پرت در ستون {column}: {outliers}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Address      0.0\n",
       "code         0.0\n",
       "purchased    0.0\n",
       "value        0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_percentage = U1400P3S05.isnull().sum() / len(U1400P3S05) * 100\n",
    "missing_percentage.sort_values(ascending=False)  \n",
    "missing_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicates = U1400P3S05.duplicated().sum()\n",
    "duplicates\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ستون‌های دسته‌بندی‌شده:\n",
      "Index([], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "categorical_columns = U1400P3S05.select_dtypes(include=['object']).columns\n",
    "print(\"ستون‌های دسته‌بندی‌شده:\")\n",
    "print(categorical_columns)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ستون‌های عددی:\n",
      "Index(['Address', 'code', 'purchased', 'value'], dtype='object')\n",
      "تعداد داده‌های پرت در ستون code: 10130\n",
      "تعداد داده‌های پرت در ستون purchased: 906\n",
      "تعداد داده‌های پرت در ستون value: 6550\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "numerical_columns = U1400P3S05.select_dtypes(include=['number']).columns\n",
    "print(\"ستون‌های عددی:\")\n",
    "print(numerical_columns)\n",
    "# بررسی داده‌های پرت بر اساس IQR\n",
    "for column in numerical_columns:\n",
    "    Q1 = U1400P3S05[column].quantile(0.25)\n",
    "    Q3 = U1400P3S05[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    outliers = ((U1400P3S05[column] < (Q1 - 1.5 * IQR)) | (U1400P3S05[column] > (Q3 + 1.5 * IQR))).sum()\n",
    "    if outliers > 0:\n",
    "        print(f\"تعداد داده‌های پرت در ستون {column}: {outliers}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Address      0.0\n",
       "code         0.0\n",
       "purchased    0.0\n",
       "value        0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_percentage = U1400P3S06.isnull().sum() / len(U1400P3S06) * 100\n",
    "missing_percentage.sort_values(ascending=False)  \n",
    "missing_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "ستون‌های دسته‌بندی‌شده:\n",
      "Index([], dtype='object')\n",
      "ستون‌های عددی:\n",
      "Index(['Address', 'code', 'purchased', 'value'], dtype='object')\n",
      "تعداد داده‌های پرت در ستون code: 5\n",
      "تعداد داده‌های پرت در ستون purchased: 2754\n",
      "تعداد داده‌های پرت در ستون value: 3499\n"
     ]
    }
   ],
   "source": [
    "duplicates = U1400P3S06.duplicated().sum()\n",
    "print(duplicates)\n",
    "categorical_columns = U1400P3S06.select_dtypes(include=['object']).columns\n",
    "print(\"ستون‌های دسته‌بندی‌شده:\")\n",
    "print(categorical_columns)\n",
    "\n",
    "numerical_columns = U1400P3S06.select_dtypes(include=['number']).columns\n",
    "print(\"ستون‌های عددی:\")\n",
    "print(numerical_columns)\n",
    "# بررسی داده‌های پرت بر اساس IQR\n",
    "for column in numerical_columns:\n",
    "    Q1 = U1400P3S06[column].quantile(0.25)\n",
    "    Q3 = U1400P3S06[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    outliers = ((U1400P3S06[column] < (Q1 - 1.5 * IQR)) | (U1400P3S06[column] > (Q3 + 1.5 * IQR))).sum()\n",
    "    if outliers > 0:\n",
    "        print(f\"تعداد داده‌های پرت در ستون {column}: {outliers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Address      0.0\n",
       "code         0.0\n",
       "purchased    0.0\n",
       "value        0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_percentage = U1400P3S07.isnull().sum() / len(U1400P3S07) * 100\n",
    "missing_percentage.sort_values(ascending=False)  \n",
    "missing_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "duplicates = U1400P3S07.duplicated().sum()\n",
    "print(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ستون‌های دسته‌بندی‌شده:\n",
      "Index([], dtype='object')\n",
      "ستون‌های عددی:\n",
      "Index(['Address', 'code', 'purchased', 'value'], dtype='object')\n",
      "تعداد داده‌های پرت در ستون purchased: 201\n",
      "تعداد داده‌های پرت در ستون value: 2090\n"
     ]
    }
   ],
   "source": [
    "categorical_columns = U1400P3S07.select_dtypes(include=['object']).columns\n",
    "print(\"ستون‌های دسته‌بندی‌شده:\")\n",
    "print(categorical_columns)\n",
    "\n",
    "numerical_columns = U1400P3S07.select_dtypes(include=['number']).columns\n",
    "print(\"ستون‌های عددی:\")\n",
    "print(numerical_columns)\n",
    "# بررسی داده‌های پرت بر اساس IQR\n",
    "for column in numerical_columns:\n",
    "    Q1 = U1400P3S07[column].quantile(0.25)\n",
    "    Q3 = U1400P3S07[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    outliers = ((U1400P3S07[column] < (Q1 - 1.5 * IQR)) | (U1400P3S07[column] > (Q3 + 1.5 * IQR))).sum()\n",
    "    if outliers > 0:\n",
    "        print(f\"تعداد داده‌های پرت در ستون {column}: {outliers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Address      0.0\n",
       "code         0.0\n",
       "purchased    0.0\n",
       "value        0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_percentage = U1400P3S05.isnull().sum() / len(U1400P3S05) * 100\n",
    "missing_percentage.sort_values(ascending=False)  \n",
    "missing_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicates = U1400P3S05.duplicated().sum()\n",
    "duplicates\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ستون‌های دسته‌بندی‌شده:\n",
      "Index([], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "categorical_columns = U1400P3S05.select_dtypes(include=['object']).columns\n",
    "print(\"ستون‌های دسته‌بندی‌شده:\")\n",
    "print(categorical_columns)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ستون‌های عددی:\n",
      "Index(['Address', 'code', 'purchased', 'value'], dtype='object')\n",
      "تعداد داده‌های پرت در ستون code: 10130\n",
      "تعداد داده‌های پرت در ستون purchased: 906\n",
      "تعداد داده‌های پرت در ستون value: 6550\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "numerical_columns = U1400P3S05.select_dtypes(include=['number']).columns\n",
    "print(\"ستون‌های عددی:\")\n",
    "print(numerical_columns)\n",
    "# بررسی داده‌های پرت بر اساس IQR\n",
    "for column in numerical_columns:\n",
    "    Q1 = U1400P3S05[column].quantile(0.25)\n",
    "    Q3 = U1400P3S05[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    outliers = ((U1400P3S05[column] < (Q1 - 1.5 * IQR)) | (U1400P3S05[column] > (Q3 + 1.5 * IQR))).sum()\n",
    "    if outliers > 0:\n",
    "        print(f\"تعداد داده‌های پرت در ستون {column}: {outliers}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Address      0.0\n",
       "code         0.0\n",
       "purchased    0.0\n",
       "value        0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_percentage = U1400P3S06.isnull().sum() / len(U1400P3S06) * 100\n",
    "missing_percentage.sort_values(ascending=False)  \n",
    "missing_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "ستون‌های دسته‌بندی‌شده:\n",
      "Index([], dtype='object')\n",
      "ستون‌های عددی:\n",
      "Index(['Address', 'code', 'purchased', 'value'], dtype='object')\n",
      "تعداد داده‌های پرت در ستون code: 5\n",
      "تعداد داده‌های پرت در ستون purchased: 2754\n",
      "تعداد داده‌های پرت در ستون value: 3499\n"
     ]
    }
   ],
   "source": [
    "duplicates = U1400P3S06.duplicated().sum()\n",
    "print(duplicates)\n",
    "categorical_columns = U1400P3S06.select_dtypes(include=['object']).columns\n",
    "print(\"ستون‌های دسته‌بندی‌شده:\")\n",
    "print(categorical_columns)\n",
    "\n",
    "numerical_columns = U1400P3S06.select_dtypes(include=['number']).columns\n",
    "print(\"ستون‌های عددی:\")\n",
    "print(numerical_columns)\n",
    "# بررسی داده‌های پرت بر اساس IQR\n",
    "for column in numerical_columns:\n",
    "    Q1 = U1400P3S06[column].quantile(0.25)\n",
    "    Q3 = U1400P3S06[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    outliers = ((U1400P3S06[column] < (Q1 - 1.5 * IQR)) | (U1400P3S06[column] > (Q3 + 1.5 * IQR))).sum()\n",
    "    if outliers > 0:\n",
    "        print(f\"تعداد داده‌های پرت در ستون {column}: {outliers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Address      0.0\n",
       "code         0.0\n",
       "purchased    0.0\n",
       "value        0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_percentage = U1400P3S07.isnull().sum() / len(U1400P3S07) * 100\n",
    "missing_percentage.sort_values(ascending=False)  \n",
    "missing_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "duplicates = U1400P3S07.duplicated().sum()\n",
    "print(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ستون‌های دسته‌بندی‌شده:\n",
      "Index([], dtype='object')\n",
      "ستون‌های عددی:\n",
      "Index(['Address', 'code', 'purchased', 'value'], dtype='object')\n",
      "تعداد داده‌های پرت در ستون purchased: 201\n",
      "تعداد داده‌های پرت در ستون value: 2090\n"
     ]
    }
   ],
   "source": [
    "categorical_columns = U1400P3S07.select_dtypes(include=['object']).columns\n",
    "print(\"ستون‌های دسته‌بندی‌شده:\")\n",
    "print(categorical_columns)\n",
    "\n",
    "numerical_columns = U1400P3S07.select_dtypes(include=['number']).columns\n",
    "print(\"ستون‌های عددی:\")\n",
    "print(numerical_columns)\n",
    "# بررسی داده‌های پرت بر اساس IQR\n",
    "for column in numerical_columns:\n",
    "    Q1 = U1400P3S07[column].quantile(0.25)\n",
    "    Q3 = U1400P3S07[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    outliers = ((U1400P3S07[column] < (Q1 - 1.5 * IQR)) | (U1400P3S07[column] > (Q3 + 1.5 * IQR))).sum()\n",
    "    if outliers > 0:\n",
    "        print(f\"تعداد داده‌های پرت در ستون {column}: {outliers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Address      0.0\n",
       "code         0.0\n",
       "purchased    0.0\n",
       "value        0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_percentage = U1400P3S08.isnull().sum() / len(U1400P3S08) * 100\n",
    "missing_percentage.sort_values(ascending=False)  \n",
    "missing_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "duplicates = U1400P3S08.duplicated().sum()\n",
    "print(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ستون‌های دسته‌بندی‌شده:\n",
      "Index([], dtype='object')\n",
      "ستون‌های عددی:\n",
      "Index(['Address', 'code', 'purchased', 'value'], dtype='object')\n",
      "تعداد داده‌های پرت در ستون code: 10513\n",
      "تعداد داده‌های پرت در ستون purchased: 15\n",
      "تعداد داده‌های پرت در ستون value: 2312\n"
     ]
    }
   ],
   "source": [
    "categorical_columns = U1400P3S08.select_dtypes(include=['object']).columns\n",
    "print(\"ستون‌های دسته‌بندی‌شده:\")\n",
    "print(categorical_columns)\n",
    "\n",
    "numerical_columns = U1400P3S08.select_dtypes(include=['number']).columns\n",
    "print(\"ستون‌های عددی:\")\n",
    "print(numerical_columns)\n",
    "# بررسی داده‌های پرت بر اساس IQR\n",
    "for column in numerical_columns:\n",
    "    Q1 = U1400P3S08[column].quantile(0.25)\n",
    "    Q3 = U1400P3S08[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    outliers = ((U1400P3S08[column] < (Q1 - 1.5 * IQR)) | (U1400P3S08[column] > (Q3 + 1.5 * IQR))).sum()\n",
    "    if outliers > 0:\n",
    "        print(f\"تعداد داده‌های پرت در ستون {column}: {outliers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Address      0.0\n",
       "code         0.0\n",
       "purchased    0.0\n",
       "value        0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_percentage = U1400P3S09.isnull().sum() / len(U1400P3S09) * 100\n",
    "missing_percentage.sort_values(ascending=False)  \n",
    "missing_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "duplicates = U1400P3S09.duplicated().sum()\n",
    "print(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ستون‌های دسته‌بندی‌شده:\n",
      "Index([], dtype='object')\n",
      "ستون‌های عددی:\n",
      "Index(['Address', 'code', 'purchased', 'value'], dtype='object')\n",
      "تعداد داده‌های پرت در ستون code: 3190\n",
      "تعداد داده‌های پرت در ستون purchased: 37\n",
      "تعداد داده‌های پرت در ستون value: 1047\n"
     ]
    }
   ],
   "source": [
    "categorical_columns = U1400P3S09.select_dtypes(include=['object']).columns\n",
    "print(\"ستون‌های دسته‌بندی‌شده:\")\n",
    "print(categorical_columns)\n",
    "\n",
    "numerical_columns = U1400P3S09.select_dtypes(include=['number']).columns\n",
    "print(\"ستون‌های عددی:\")\n",
    "print(numerical_columns)\n",
    "# بررسی داده‌های پرت بر اساس IQR\n",
    "for column in numerical_columns:\n",
    "    Q1 = U1400P3S09[column].quantile(0.25)\n",
    "    Q3 = U1400P3S09[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    outliers = ((U1400P3S09[column] < (Q1 - 1.5 * IQR)) | (U1400P3S09[column] > (Q3 + 1.5 * IQR))).sum()\n",
    "    if outliers > 0:\n",
    "        print(f\"تعداد داده‌های پرت در ستون {column}: {outliers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Address      0.0\n",
       "code         0.0\n",
       "purchased    0.0\n",
       "value        0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_percentage = U1400P3S11.isnull().sum() / len(U1400P3S11) * 100\n",
    "missing_percentage.sort_values(ascending=False)  \n",
    "missing_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "duplicates = U1400P3S11.duplicated().sum()\n",
    "print(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ستون‌های دسته‌بندی‌شده:\n",
      "Index([], dtype='object')\n",
      "ستون‌های عددی:\n",
      "Index(['Address', 'code', 'purchased', 'value'], dtype='object')\n",
      "تعداد داده‌های پرت در ستون code: 192\n",
      "تعداد داده‌های پرت در ستون purchased: 302\n",
      "تعداد داده‌های پرت در ستون value: 1807\n"
     ]
    }
   ],
   "source": [
    "categorical_columns = U1400P3S11.select_dtypes(include=['object']).columns\n",
    "print(\"ستون‌های دسته‌بندی‌شده:\")\n",
    "print(categorical_columns)\n",
    "\n",
    "numerical_columns = U1400P3S11.select_dtypes(include=['number']).columns\n",
    "print(\"ستون‌های عددی:\")\n",
    "print(numerical_columns)\n",
    "# بررسی داده‌های پرت بر اساس IQR\n",
    "for column in numerical_columns:\n",
    "    Q1 = U1400P3S11[column].quantile(0.25)\n",
    "    Q3 = U1400P3S11[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    outliers = ((U1400P3S11[column] < (Q1 - 1.5 * IQR)) | (U1400P3S11[column] > (Q3 + 1.5 * IQR))).sum()\n",
    "    if outliers > 0:\n",
    "        print(f\"تعداد داده‌های پرت در ستون {column}: {outliers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "duplicates = U1400P3S12.duplicated().sum()\n",
    "print(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ستون‌های دسته‌بندی‌شده:\n",
      "Index([], dtype='object')\n",
      "ستون‌های عددی:\n",
      "Index(['Address', 'code', 'purchased', 'value'], dtype='object')\n",
      "تعداد داده‌های پرت در ستون code: 819\n",
      "تعداد داده‌های پرت در ستون purchased: 671\n",
      "تعداد داده‌های پرت در ستون value: 5701\n"
     ]
    }
   ],
   "source": [
    "categorical_columns = U1400P3S12.select_dtypes(include=['object']).columns\n",
    "print(\"ستون‌های دسته‌بندی‌شده:\")\n",
    "print(categorical_columns)\n",
    "\n",
    "numerical_columns = U1400P3S12.select_dtypes(include=['number']).columns\n",
    "print(\"ستون‌های عددی:\")\n",
    "print(numerical_columns)\n",
    "# بررسی داده‌های پرت بر اساس IQR\n",
    "for column in numerical_columns:\n",
    "    Q1 = U1400P3S12[column].quantile(0.25)\n",
    "    Q3 = U1400P3S12[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    outliers = ((U1400P3S12[column] < (Q1 - 1.5 * IQR)) | (U1400P3S12[column] > (Q3 + 1.5 * IQR))).sum()\n",
    "    if outliers > 0:\n",
    "        print(f\"تعداد داده‌های پرت در ستون {column}: {outliers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Address    0.000000\n",
       "code       0.000000\n",
       "value      0.263589\n",
       "dtype: float64"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_percentage = U1400P3S13.isnull().sum() / len(U1400P3S13) * 100\n",
    "missing_percentage.sort_values(ascending=False)  \n",
    "missing_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "U1400P3S13 = U1400P3S13.dropna(subset=['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "410\n"
     ]
    }
   ],
   "source": [
    "duplicates = U1400P3S13.duplicated().sum()\n",
    "print(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "U1400P3S13 = U1400P3S13.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "duplicates = U1400P3S13.duplicated().sum()\n",
    "print(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ستون‌های دسته‌بندی‌شده:\n",
      "Index(['value'], dtype='object')\n",
      "ستون‌های عددی:\n",
      "Index(['Address', 'code'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "categorical_columns = U1400P3S13.select_dtypes(include=['object']).columns\n",
    "print(\"ستون‌های دسته‌بندی‌شده:\")\n",
    "print(categorical_columns)\n",
    "\n",
    "numerical_columns = U1400P3S13.select_dtypes(include=['number']).columns\n",
    "print(\"ستون‌های عددی:\")\n",
    "print(numerical_columns)\n",
    "# بررسی داده‌های پرت بر اساس IQR\n",
    "for column in numerical_columns:\n",
    "    Q1 = U1400P3S13[column].quantile(0.25)\n",
    "    Q3 = U1400P3S13[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    outliers = ((U1400P3S13[column] < (Q1 - 1.5 * IQR)) | (U1400P3S13[column] > (Q3 + 1.5 * IQR))).sum()\n",
    "    if outliers > 0:\n",
    "        print(f\"تعداد داده‌های پرت در ستون {column}: {outliers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Address      0.000000\n",
       "code         0.000000\n",
       "purchased    4.468169\n",
       "value        4.784500\n",
       "dtype: float64"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_percentage = U1400P3S14.isnull().sum() / len(U1400P3S14) * 100\n",
    "missing_percentage.sort_values(ascending=False)  \n",
    "missing_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "U1400P3S14 = U1400P3S14.dropna(subset=['purchased','value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Address      0.0\n",
       "code         0.0\n",
       "purchased    0.0\n",
       "value        0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_percentage = U1400P3S14.isnull().sum() / len(U1400P3S14) * 100\n",
    "missing_percentage.sort_values(ascending=False)  \n",
    "missing_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "duplicates = U1400P3S14.duplicated().sum()\n",
    "print(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ستون‌های دسته‌بندی‌شده:\n",
      "Index(['purchased'], dtype='object')\n",
      "ستون‌های عددی:\n",
      "Index(['Address', 'code', 'value'], dtype='object')\n",
      "تعداد داده‌های پرت در ستون value: 394\n"
     ]
    }
   ],
   "source": [
    "categorical_columns = U1400P3S14.select_dtypes(include=['object']).columns\n",
    "print(\"ستون‌های دسته‌بندی‌شده:\")\n",
    "print(categorical_columns)\n",
    "\n",
    "numerical_columns = U1400P3S14.select_dtypes(include=['number']).columns\n",
    "print(\"ستون‌های عددی:\")\n",
    "print(numerical_columns)\n",
    "# بررسی داده‌های پرت بر اساس IQR\n",
    "for column in numerical_columns:\n",
    "    Q1 = U1400P3S14[column].quantile(0.25)\n",
    "    Q3 = U1400P3S14[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    outliers = ((U1400P3S14[column] < (Q1 - 1.5 * IQR)) | (U1400P3S14[column] > (Q3 + 1.5 * IQR))).sum()\n",
    "    if outliers > 0:\n",
    "        print(f\"تعداد داده‌های پرت در ستون {column}: {outliers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Address            0.000000\n",
       "member             0.000000\n",
       "employed_w         0.000000\n",
       "ISCO_w             0.000000\n",
       "ISIC_w             0.000000\n",
       "status_w           0.000000\n",
       "hours_w            4.774579\n",
       "days_w             4.758448\n",
       "income_w_m         0.000000\n",
       "income_w_y         0.000000\n",
       "wage_w_m           0.024195\n",
       "wage_w_y           0.008065\n",
       "perk_w_m           0.008065\n",
       "perk_w_y           0.008065\n",
       "netincome_w_m      0.000000\n",
       "netincome_w_y      0.000000\n",
       "Fasl               0.000000\n",
       "year               0.000000\n",
       "DYCOL00          100.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_percentage = U1400P4S01.isnull().sum() / len(U1400P4S01) * 100\n",
    "missing_percentage.sort_values(ascending=False)  \n",
    "missing_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "U1400P4S01 = U1400P4S01.drop(columns='DYCOL00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "U1400P4S01_cleaned = U1400P4S01.dropna(subset=['hours_w','days_w','wage_w_m','wage_w_y','perk_w_m','perk_w_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Address          0.0\n",
       "member           0.0\n",
       "employed_w       0.0\n",
       "ISCO_w           0.0\n",
       "ISIC_w           0.0\n",
       "status_w         0.0\n",
       "hours_w          0.0\n",
       "days_w           0.0\n",
       "income_w_m       0.0\n",
       "income_w_y       0.0\n",
       "wage_w_m         0.0\n",
       "wage_w_y         0.0\n",
       "perk_w_m         0.0\n",
       "perk_w_y         0.0\n",
       "netincome_w_m    0.0\n",
       "netincome_w_y    0.0\n",
       "Fasl             0.0\n",
       "year             0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_percentage = U1400P4S01_cleaned.isnull().sum() / len(U1400P4S01_cleaned) * 100\n",
    "missing_percentage.sort_values(ascending=False)  \n",
    "missing_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Address              0.000000\n",
       "member               0.000000\n",
       "employed_s           0.000000\n",
       "ISCO_s               0.015684\n",
       "ISIC_s               0.015684\n",
       "status_s             0.000000\n",
       "agriculture          0.000000\n",
       "hours_s              1.505646\n",
       "days_s               1.505646\n",
       "cost_employment     15.040778\n",
       "cost_raw             4.548306\n",
       "cost_machinery       9.190715\n",
       "cost_others          6.618570\n",
       "cost_tax            15.824969\n",
       "sale                 1.270389\n",
       "income_s_y           0.000000\n",
       "Fasl                 0.000000\n",
       "year                 0.000000\n",
       "DYCOL00            100.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_percentage = U1400P4S02.isnull().sum() / len(U1400P4S02) * 100\n",
    "missing_percentage.sort_values(ascending=False)  \n",
    "missing_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "U1400P4S02 = U1400P4S02.drop(columns='DYCOL00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "U1400P4S02_cleaned = U1400P4S02.dropna(subset=['member','employed_s','ISCO_s','ISIC_s','hours_s','cost_raw','cost_others','sale'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Address            0.000000\n",
       "member             0.000000\n",
       "employed_s         0.000000\n",
       "ISCO_s             0.000000\n",
       "ISIC_s             0.000000\n",
       "status_s           0.000000\n",
       "agriculture        0.000000\n",
       "hours_s            0.000000\n",
       "days_s             0.000000\n",
       "cost_employment    9.156627\n",
       "cost_raw           0.000000\n",
       "cost_machinery     3.769363\n",
       "cost_others        0.000000\n",
       "cost_tax           9.432014\n",
       "sale               0.000000\n",
       "income_s_y         0.000000\n",
       "Fasl               0.000000\n",
       "year               0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_percentage = U1400P4S02_cleaned.isnull().sum() / len(U1400P4S02_cleaned) * 100\n",
    "missing_percentage.sort_values(ascending=False)  \n",
    "missing_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "U1400P4S02_cleaned = U1400P4S02.dropna(subset=['cost_employment','cost_machinery','cost_tax'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Address              0.000000\n",
       "member               0.004108\n",
       "income_pension      58.907373\n",
       "income_rent         73.555145\n",
       "income_interest     36.463339\n",
       "income_aid          28.186486\n",
       "income_resale       76.023824\n",
       "income_transfer     64.481413\n",
       "Fasl                 0.000000\n",
       "year                 0.000000\n",
       "DYCOL00            100.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_percentage = U1400P4S03.isnull().sum() / len(U1400P4S03) * 100\n",
    "missing_percentage.sort_values(ascending=False)  \n",
    "missing_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "U1400P4S03 = U1400P4S03.drop(columns='DYCOL00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "U1400P4S03_cleaned = U1400P4S03.dropna(subset=['member'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Address              int64\n",
      "member             float64\n",
      "income_pension      object\n",
      "income_rent        float64\n",
      "income_interest    float64\n",
      "income_aid         float64\n",
      "income_resale      float64\n",
      "income_transfer    float64\n",
      "Fasl                 int64\n",
      "year                 int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(U1400P4S03_cleaned.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "U1400P4S03_cleaned = U1400P4S03_cleaned.apply(pd.to_numeric, errors='coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded7 = U1400P4S03_cleaned.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for income_aid : 0.21332570774949958\n"
     ]
    }
   ],
   "source": [
    "df_not_null = df_encoded7[df_encoded7['income_aid'].notnull()]\n",
    "df_null = df_encoded7[df_encoded7['income_aid'].isnull()]\n",
    "\n",
    "\n",
    "X = df_not_null.drop(['income_aid'], axis=1)\n",
    "y = df_not_null['income_aid']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "print(f'Accuracy for income_aid : {accuracy_score(y_test, y_pred)}')\n",
    "\n",
    "\n",
    "X_null = df_null.drop(['income_aid'], axis=1)\n",
    "\n",
    "\n",
    "df_null.loc[:, 'income_aid'] = model.predict(X_null)\n",
    "\n",
    "\n",
    "U1400P4S03_filled = pd.concat([df_not_null, df_null])\n",
    "\n",
    "\n",
    "U1400P4S03_filled.sort_index(inplace=True)\n",
    "\n",
    "\n",
    "#print(U1400P4S031_filled.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Address             0.000000\n",
       "member              0.000000\n",
       "subsidy_number      0.006239\n",
       "subsidy_month       0.006239\n",
       "subsidy             0.000000\n",
       "Fasl                0.000000\n",
       "year                0.000000\n",
       "DYCOL00           100.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_percentage = U1400P4S04.isnull().sum() / len(U1400P4S04) * 100\n",
    "missing_percentage.sort_values(ascending=False)  \n",
    "missing_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "U1400P4S04 = U1400P4S04.drop(columns='DYCOL00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "U1400P4S04_cleaned = U1400P4S04.dropna(subset=['subsidy_number','subsidy_month','subsidy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_frames = {\n",
    "    'U1400Data': U1400Data_cleaned,\n",
    "    'U1400P1': U1400P1_filled,\n",
    "    'U1400P2': U1400P2,\n",
    "    'Food': U1400P3S01_cleaned,\n",
    "    'Tobacco': U1400P3S02_cleaned,\n",
    "    'Clothing': U1400P3S03,\n",
    "    'Housing': U1400P3S04_cleaned,\n",
    "    'Furniture': U1400P3S05,\n",
    "    'Health': U1400P3S06,\n",
    "    'Transport': U1400P3S07,\n",
    "    'Communication': U1400P3S08,\n",
    "    'Recreation': U1400P3S09,\n",
    "    'Education': U1400P3S11,\n",
    "    'Hotel': U1400P3S12,\n",
    "    'Miscellaneous': U1400P3S13,\n",
    "    'Investment': U1400P3S14,\n",
    "    'U1400P4S01': U1400P4S01_cleaned,\n",
    "    'U1400P4S02': U1400P4S02_cleaned,\n",
    "    'U1400P4S03': U1400P4S03_cleaned,\n",
    "    'U1400P4S04': U1400P4S04_cleaned\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with pd.ExcelWriter('cleaned_data_final_U1400.xlsx') as writer:\n",
    "    for sheet_name, df in data_frames.items():\n",
    "        df.to_excel(writer, sheet_name=sheet_name, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
