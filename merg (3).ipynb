{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the output of the feature selection section, we first merge the selected features for all years. As shown in the code, the 'urban' column (to indicate rural or urban data as 0 and 1) and the 'year' column have been added to the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Address  MahMorajeh  Fasl    weight  khanevartype  Takmil  town  \\\n",
      "0  20001383908         5.0     2  5.148654             1       1     1   \n",
      "1  20001383908         5.0     2  5.148654             1       1     1   \n",
      "2  20001383908         5.0     2  5.148654             1       1     1   \n",
      "3  20001383908         5.0     2  5.148654             1       1     1   \n",
      "4  20001383911         5.0     2  5.148654             1       2     1   \n",
      "\n",
      "   province_encoded  Urban  Year  ...  value_Expenditure  value_invesments  \\\n",
      "0               0.0  False  1398  ...         63213552.0               NaN   \n",
      "1               0.0  False  1398  ...         63213552.0               NaN   \n",
      "2               0.0  False  1398  ...         63213552.0               NaN   \n",
      "3               0.0  False  1398  ...         63213552.0               NaN   \n",
      "4               0.0  False  1398  ...         38560000.0               NaN   \n",
      "\n",
      "   value_vehicle  income_s_y  income_pension  income_rent  income_interest  \\\n",
      "0      5100000.0         NaN             NaN          NaN              NaN   \n",
      "1      5100000.0         NaN             NaN          NaN              NaN   \n",
      "2      5100000.0         NaN             NaN          NaN              NaN   \n",
      "3      5100000.0         NaN             NaN          NaN              NaN   \n",
      "4            NaN         NaN             0.0          0.0              0.0   \n",
      "\n",
      "   income_aid  income_resale  income_transfer  \n",
      "0         NaN            NaN              NaN  \n",
      "1         NaN            NaN              NaN  \n",
      "2         NaN            NaN              NaN  \n",
      "3         NaN            NaN              NaN  \n",
      "4  33000000.0            0.0       10000000.0  \n",
      "\n",
      "[5 rows x 45 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def empty_string_remover(df):\n",
    "    return df.replace('', pd.NA)\n",
    "\n",
    "def add_suffix_to_columns(df, suffix):\n",
    "    df.columns = [f\"{col}_{suffix}\" if col != \"Address\" else col for col in df.columns]\n",
    "    return df\n",
    "\n",
    "def read_available_columns(file, sheet_name, desired_columns):\n",
    "    all_columns = pd.read_excel(file, sheet_name=sheet_name, nrows=0).columns\n",
    "    available_columns = [col for col in desired_columns if col in all_columns]\n",
    "    return pd.read_excel(file, sheet_name=sheet_name, usecols=available_columns)\n",
    "\n",
    "sheet_indices = [0, 1, 2, 9, 14, 15, 17, 18, 19]\n",
    "\n",
    "files = ['R98_cleaned.xlsx', 'U98_cleaned.xlsx', 'R99_cleaned.xlsx', 'U99_cleaned.xlsx', 'R1400_cleaned.xlsx', 'U1400_cleaned.xlsx', 'R1401_cleaned.xlsx', 'U1401_cleaned.xlsx']\n",
    "\n",
    "sheets_desc = {\n",
    "    0: \"R99Data\",\n",
    "    1: \"R99P1\",\n",
    "    2: \"R99P2\",\n",
    "    9: \"Transport\",\n",
    "    14: \"Expenditure\",\n",
    "    15: \"invesments\",\n",
    "    17: \"R99P4S01\",\n",
    "    18: \"R99P4S02\",\n",
    "    19: \"R99P4S03\",\n",
    "}\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "dfs = []\n",
    "\n",
    "for file in files:\n",
    "    file_path = os.path.join(current_directory, file)\n",
    "    year_file = pd.ExcelFile(file_path, engine='openpyxl')\n",
    "\n",
    "    Urban = \"U\" in file\n",
    "    year = int(re.findall(r'\\d+', file.split('.')[0])[0])\n",
    "    if year < 100:\n",
    "        year += 1300\n",
    "\n",
    "    year_df = pd.DataFrame()\n",
    "\n",
    "    for sheet_index in sheet_indices:\n",
    "        sheet_name = year_file.sheet_names[sheet_index]\n",
    "\n",
    "        if sheet_index == 0:\n",
    "            desired_columns = [\"Address\", \"MahMorajeh\", \"Fasl\", \"weight\", \"Takmil\", \"khanevartype\", \"town\", \"province_encoded\"]\n",
    "            current_df = read_available_columns(year_file, sheet_name, desired_columns)\n",
    "            current_df = empty_string_remover(current_df)\n",
    "            current_df[\"Urban\"] = Urban\n",
    "            current_df[\"Year\"] = year\n",
    "            year_df = current_df.copy()\n",
    "\n",
    "        elif sheet_index == 1:\n",
    "            desired_columns = [\"Address\", \"member\", \"age\", \"relation_encoded\", \"gender_encoded\", \"studying_encoded\", \"degree_encoded\", \"occupationalst_encoded\", \"maritalst_encoded\"]\n",
    "            current_df = read_available_columns(year_file, sheet_name, desired_columns)\n",
    "            current_df = empty_string_remover(current_df)\n",
    "\n",
    "            family_member_count = current_df.groupby(\"Address\").agg({\"member\": \"count\"}).reset_index()\n",
    "            active_member_count = current_df[current_df.occupationalst_encoded == 1].groupby(\"Address\").agg({\"occupationalst_encoded\": \"count\"}).reset_index()\n",
    "            \n",
    "            year_df = year_df.merge(current_df, on=\"Address\", how=\"left\")\n",
    "            year_df = year_df.merge(family_member_count, on=\"Address\", how=\"left\")\n",
    "            year_df = year_df.merge(active_member_count, on=\"Address\", how=\"left\")\n",
    "\n",
    "        elif sheet_index == 2:\n",
    "            desired_columns = [\"Address\", \"vehicle\", \"room\", \"space\", \"construction\", \"refridgerator\", \"fridge\", \"sewingmachine\", \"fan\", \"telephone\", \"evapcooling\", \"tenure_encoded\", \"Material_encoded\", \"HeatingFuel_encoded\", \"waterheatingfuel_encoded\"]\n",
    "            current_df = read_available_columns(year_file, sheet_name, desired_columns)\n",
    "            current_df = empty_string_remover(current_df)\n",
    "            year_df = year_df.merge(current_df, on=\"Address\", how=\"left\")\n",
    "\n",
    "        elif sheet_index == 9:\n",
    "            current_df = pd.read_excel(year_file, sheet_name=sheet_name)\n",
    "            current_df = empty_string_remover(current_df)\n",
    "            transportation_cost_df = current_df.groupby(\"Address\").agg({\"value\": \"sum\"}).reset_index()\n",
    "            transportation_cost_df = add_suffix_to_columns(transportation_cost_df, \"Transport\")\n",
    "            year_df = year_df.merge(transportation_cost_df, on=\"Address\", how=\"left\")\n",
    "\n",
    "        elif sheet_index == 14:\n",
    "            current_df = pd.read_excel(year_file, sheet_name=sheet_name)\n",
    "            current_df = empty_string_remover(current_df)\n",
    "            current_df['value'] = pd.to_numeric(current_df['value'], errors='coerce').fillna(0)\n",
    "            expenditure_df = current_df.groupby(\"Address\").agg({\"value\": \"sum\"}).reset_index()\n",
    "            expenditure_df = add_suffix_to_columns(expenditure_df, \"Expenditure\")\n",
    "            year_df = year_df.merge(expenditure_df, on=\"Address\", how=\"left\")\n",
    "\n",
    "            insurance_expenses = current_df[current_df.code.isin([125411, 125412, 125413])].groupby(\"Address\").agg({\"value\": \"sum\"}).reset_index()\n",
    "            vehicle_expenses = current_df[current_df.code.astype(\"str\").str.match(\"^7[12]\\\\d+$\")].groupby(\"Address\").agg({\"value\": \"sum\"}).reset_index()\n",
    "            insurance_expenses = add_suffix_to_columns(insurance_expenses, \"invesments\")\n",
    "            vehicle_expenses = add_suffix_to_columns(vehicle_expenses, \"vehicle\")\n",
    "            year_df = year_df.merge(insurance_expenses, on=\"Address\", how=\"left\")\n",
    "            year_df = year_df.merge(vehicle_expenses, on=\"Address\", how=\"left\")\n",
    "\n",
    "        elif sheet_index == 17:\n",
    "            current_df = pd.read_excel(year_file, sheet_name=sheet_name)\n",
    "            current_df = empty_string_remover(current_df)\n",
    "            income_azad_df = current_df.groupby(\"Address\").agg({\"income_s_y\": \"sum\"}).reset_index()\n",
    "            year_df = year_df.merge(income_azad_df, on=\"Address\", how=\"left\")\n",
    "\n",
    "        elif sheet_index == 18:\n",
    "            current_df = pd.read_excel(year_file, sheet_name=sheet_name)\n",
    "            current_df = empty_string_remover(current_df)\n",
    "            misc_income = current_df.groupby(\"Address\").agg({\n",
    "                \"income_pension\": \"sum\",\n",
    "                \"income_rent\": \"sum\",\n",
    "                \"income_interest\": \"sum\",\n",
    "                \"income_aid\": \"sum\",\n",
    "                \"income_resale\": \"sum\",\n",
    "                \"income_transfer\": \"sum\"\n",
    "            }).reset_index()\n",
    "            year_df = year_df.merge(misc_income, on=\"Address\", how=\"left\")\n",
    "\n",
    "    dfs.append(year_df)\n",
    "\n",
    "final_df = pd.concat(dfs, ignore_index=True)\n",
    "final_df.to_excel(\"final_merged_data.xlsx\", index=False)\n",
    "print(final_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Address', 'MahMorajeh', 'Fasl', 'weight', 'khanevartype', 'Takmil',\n",
       "       'town', 'province_encoded', 'Urban', 'Year', 'member_x', 'age',\n",
       "       'relation_encoded', 'gender_encoded', 'studying_encoded',\n",
       "       'degree_encoded', 'occupationalst_encoded_x', 'maritalst_encoded',\n",
       "       'member_y', 'occupationalst_encoded_y', 'room', 'space', 'construction',\n",
       "       'vehicle', 'refridgerator', 'fridge', 'sewingmachine', 'fan',\n",
       "       'telephone', 'evapcooling', 'tenure_encoded', 'Material_encoded',\n",
       "       'HeatingFuel_encoded', 'waterheatingfuel_encoded', 'value_Transport',\n",
       "       'value_Expenditure', 'value_invesments', 'value_vehicle', 'income_s_y',\n",
       "       'income_pension', 'income_rent', 'income_interest', 'income_aid',\n",
       "       'income_resale', 'income_transfer'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "در ابتدا داده تست که شامل داده های مربوط به زمستان ۱۴۰۱ است را جدا می‌کنیم"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "winter_1401_df = final_df[(final_df['Fasl'] == 4) & (final_df['Year'] == 1401)]\n",
    "\n",
    "\n",
    "winter_1401_df.to_excel(\"Testdata.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in winter_1401_df (Test set): 31207\n",
      "Rows in final_df (Before removal): 516888\n"
     ]
    }
   ],
   "source": [
    "print(f\"Rows in winter_1401_df (Test set): {len(winter_1401_df)}\")\n",
    "print(f\"Rows in final_df (Before removal): {len(final_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df[~((final_df['Fasl'] == 4) & (final_df['Year'] == 1401))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in final_df (After removal): 485681\n"
     ]
    }
   ],
   "source": [
    "print(f\"Rows in final_df (After removal): {len(final_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MahMorajeh                  46.158899\n",
      "relation_encoded            10.499690\n",
      "occupationalst_encoded_y    13.133106\n",
      "tenure_encoded              42.531826\n",
      "Material_encoded            42.531826\n",
      "HeatingFuel_encoded         42.532856\n",
      "waterheatingfuel_encoded    42.531826\n",
      "value_Transport              6.154451\n",
      "value_invesments            60.671305\n",
      "value_vehicle               50.858279\n",
      "income_s_y                  66.562414\n",
      "income_pension              13.857038\n",
      "income_rent                 13.857038\n",
      "income_interest             13.857038\n",
      "income_aid                  13.857038\n",
      "income_resale               13.857038\n",
      "income_transfer             13.857038\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# محاسبه درصد نال برای هر ستون\n",
    "percent_null = (final_df.isna().sum() / len(final_df)) * 100\n",
    "threshold = 5\n",
    "columns_above_threshold = percent_null[percent_null > threshold]\n",
    "print(columns_above_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# پر کردن نال‌ها در province_encoded با استفاده از town\n",
    "province_by_town = final_df.dropna(subset=['province_encoded']).groupby('town')['province_encoded'].apply(lambda x: x.mode()[0]).to_dict()\n",
    "\n",
    "\n",
    "final_df['province_encoded'] = final_df.apply(\n",
    "    lambda row: province_by_town[row['town']] if pd.isna(row['province_encoded']) else row['province_encoded'],\n",
    "    axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# پر کردن نال‌ها در relation_encoded با استفاده از khanevartype\n",
    "relation_by_khanevartype = final_df.dropna(subset=['relation_encoded']).groupby('khanevartype')['relation_encoded'].apply(lambda x: x.mode()[0]).to_dict()\n",
    "\n",
    "final_df['relation_encoded'] = final_df.apply(\n",
    "    lambda row: relation_by_khanevartype[row['khanevartype']] if pd.isna(row['relation_encoded']) else row['relation_encoded'],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# پر کردن نال‌ها در occupationalst_encoded_y با استفاده از gender و age\n",
    "occupation_by_gender_age = final_df.dropna(subset=['occupationalst_encoded_y']).groupby(['gender_encoded', 'age'])['occupationalst_encoded_y'].apply(lambda x: x.mode()[0]).to_dict()\n",
    "\n",
    "\n",
    "final_df['occupationalst_encoded_y'] = final_df.apply(\n",
    "    lambda row: occupation_by_gender_age.get((row['gender_encoded'], row['age']), row['occupationalst_encoded_y']),\n",
    "    axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#حذف مقادیر نال هزینه رفت و آمد(پایین بودن درصد این مقدار به ما این امکان را داد)\n",
    "filtered_df = final_df.dropna(subset=['value_Transport'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MahMorajeh                  46.014173\n",
      "tenure_encoded              42.761359\n",
      "Material_encoded            42.761359\n",
      "HeatingFuel_encoded         42.762456\n",
      "waterheatingfuel_encoded    42.761359\n",
      "value_invesments            58.166919\n",
      "value_vehicle               47.771342\n",
      "income_s_y                  65.633954\n",
      "income_pension              14.065688\n",
      "income_rent                 14.065688\n",
      "income_interest             14.065688\n",
      "income_aid                  14.065688\n",
      "income_resale               14.065688\n",
      "income_transfer             14.065688\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# محاسبه درصد نال برای هر ستون\n",
    "percent_null = (filtered_df.isna().sum() / len(filtered_df)) * 100\n",
    "threshold = 5\n",
    "columns_above_threshold = percent_null[percent_null > threshold]\n",
    "print(columns_above_threshold)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "به علت بالا  بودن مقادیر نال برخی از ستون ها برای جلوگیری از هرگونه سوگیری مدل آنها را حذف می‌کنیم"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = filtered_df.drop(columns=['income_s_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MahMorajeh                  46.014173\n",
      "tenure_encoded              42.761359\n",
      "Material_encoded            42.761359\n",
      "HeatingFuel_encoded         42.762456\n",
      "waterheatingfuel_encoded    42.761359\n",
      "value_invesments            58.166919\n",
      "value_vehicle               47.771342\n",
      "income_pension              14.065688\n",
      "income_rent                 14.065688\n",
      "income_interest             14.065688\n",
      "income_aid                  14.065688\n",
      "income_resale               14.065688\n",
      "income_transfer             14.065688\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# محاسبه درصد نال برای هر ستون\n",
    "percent_null = (filtered_df.isna().sum() / len(filtered_df)) * 100\n",
    "threshold = 5\n",
    "columns_above_threshold = percent_null[percent_null > threshold]\n",
    "print(columns_above_threshold)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "فایل را جهت همکاری موازی تا به اینجا ذخیره می‌کنیم"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to final_dataset.xlsx\n"
     ]
    }
   ],
   "source": [
    "\n",
    "file_path = 'final_dataset.xlsx'\n",
    "\n",
    "# ذخیره DataFrame به فایل اکسل\n",
    "filtered_df.to_excel(file_path, index=False)\n",
    "\n",
    "# چاپ پیام تأیید\n",
    "print(f\"Data saved to {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "یکی از روش های پر کردن مقادیر نال، استفاده ‌از knn imputer و mice میباشد.\n",
    "به علت زمانبر بودن این مدل،با وجود ۹ ساعت زماندهی، متاسفانه به خروجی آن نتوانستیم دست پیدا کنیم.\n",
    "مطمئنا استفاده از مدلهای پیچیده تر این اممکان را به ما میداد که مدل رگرسیون بهتری را آموزش دهیم"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/saraoliay/Downloads/project-quera copy/merg.ipynb Cell 22\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/saraoliay/Downloads/project-quera%20copy/merg.ipynb#X41sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mimpute\u001b[39;00m \u001b[39mimport\u001b[39;00m KNNImputer\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/saraoliay/Downloads/project-quera%20copy/merg.ipynb#X41sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m imputer \u001b[39m=\u001b[39m KNNImputer(n_neighbors\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/saraoliay/Downloads/project-quera%20copy/merg.ipynb#X41sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m filtered_df[:] \u001b[39m=\u001b[39m imputer\u001b[39m.\u001b[39;49mfit_transform(filtered_df)\n",
      "File \u001b[0;32m~/miniconda3/envs/quera/lib/python3.12/site-packages/sklearn/utils/_set_output.py:313\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[1;32m    312\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 313\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    314\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    315\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    316\u001b[0m         return_tuple \u001b[39m=\u001b[39m (\n\u001b[1;32m    317\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[1;32m    318\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[1;32m    319\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/quera/lib/python3.12/site-packages/sklearn/base.py:1098\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m   1083\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1084\u001b[0m             (\n\u001b[1;32m   1085\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThis object (\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m) has a `transform`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1093\u001b[0m             \u001b[39mUserWarning\u001b[39;00m,\n\u001b[1;32m   1094\u001b[0m         )\n\u001b[1;32m   1096\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1097\u001b[0m     \u001b[39m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m-> 1098\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(X, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\u001b[39m.\u001b[39;49mtransform(X)\n\u001b[1;32m   1099\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1100\u001b[0m     \u001b[39m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m   1101\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m~/miniconda3/envs/quera/lib/python3.12/site-packages/sklearn/utils/_set_output.py:313\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[1;32m    312\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 313\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    314\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    315\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    316\u001b[0m         return_tuple \u001b[39m=\u001b[39m (\n\u001b[1;32m    317\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[1;32m    318\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[1;32m    319\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/quera/lib/python3.12/site-packages/sklearn/impute/_knn.py:367\u001b[0m, in \u001b[0;36mKNNImputer.transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[39m# process in fixed-memory chunks\u001b[39;00m\n\u001b[1;32m    359\u001b[0m gen \u001b[39m=\u001b[39m pairwise_distances_chunked(\n\u001b[1;32m    360\u001b[0m     X[row_missing_idx, :],\n\u001b[1;32m    361\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_X,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    365\u001b[0m     reduce_func\u001b[39m=\u001b[39mprocess_chunk,\n\u001b[1;32m    366\u001b[0m )\n\u001b[0;32m--> 367\u001b[0m \u001b[39mfor\u001b[39;49;00m chunk \u001b[39min\u001b[39;49;00m gen:\n\u001b[1;32m    368\u001b[0m     \u001b[39m# process_chunk modifies X in place. No return value.\u001b[39;49;00m\n\u001b[1;32m    369\u001b[0m     \u001b[39mpass\u001b[39;49;00m\n\u001b[1;32m    371\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeep_empty_features:\n",
      "File \u001b[0;32m~/miniconda3/envs/quera/lib/python3.12/site-packages/sklearn/metrics/pairwise.py:2172\u001b[0m, in \u001b[0;36mpairwise_distances_chunked\u001b[0;34m(X, Y, reduce_func, metric, n_jobs, working_memory, **kwds)\u001b[0m\n\u001b[1;32m   2170\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2171\u001b[0m     X_chunk \u001b[39m=\u001b[39m X[sl]\n\u001b[0;32m-> 2172\u001b[0m D_chunk \u001b[39m=\u001b[39m pairwise_distances(X_chunk, Y, metric\u001b[39m=\u001b[39;49mmetric, n_jobs\u001b[39m=\u001b[39;49mn_jobs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m   2173\u001b[0m \u001b[39mif\u001b[39;00m (X \u001b[39mis\u001b[39;00m Y \u001b[39mor\u001b[39;00m Y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m PAIRWISE_DISTANCE_FUNCTIONS\u001b[39m.\u001b[39mget(\n\u001b[1;32m   2174\u001b[0m     metric, \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   2175\u001b[0m ) \u001b[39mis\u001b[39;00m euclidean_distances:\n\u001b[1;32m   2176\u001b[0m     \u001b[39m# zeroing diagonal, taking care of aliases of \"euclidean\",\u001b[39;00m\n\u001b[1;32m   2177\u001b[0m     \u001b[39m# i.e. \"l2\"\u001b[39;00m\n\u001b[1;32m   2178\u001b[0m     D_chunk\u001b[39m.\u001b[39mflat[sl\u001b[39m.\u001b[39mstart :: _num_samples(X) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/quera/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    214\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/quera/lib/python3.12/site-packages/sklearn/metrics/pairwise.py:2375\u001b[0m, in \u001b[0;36mpairwise_distances\u001b[0;34m(X, Y, metric, n_jobs, force_all_finite, **kwds)\u001b[0m\n\u001b[1;32m   2372\u001b[0m         \u001b[39mreturn\u001b[39;00m distance\u001b[39m.\u001b[39msquareform(distance\u001b[39m.\u001b[39mpdist(X, metric\u001b[39m=\u001b[39mmetric, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds))\n\u001b[1;32m   2373\u001b[0m     func \u001b[39m=\u001b[39m partial(distance\u001b[39m.\u001b[39mcdist, metric\u001b[39m=\u001b[39mmetric, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m-> 2375\u001b[0m \u001b[39mreturn\u001b[39;00m _parallel_pairwise(X, Y, func, n_jobs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n",
      "File \u001b[0;32m~/miniconda3/envs/quera/lib/python3.12/site-packages/sklearn/metrics/pairwise.py:1893\u001b[0m, in \u001b[0;36m_parallel_pairwise\u001b[0;34m(X, Y, func, n_jobs, **kwds)\u001b[0m\n\u001b[1;32m   1890\u001b[0m X, Y, dtype \u001b[39m=\u001b[39m _return_float_dtype(X, Y)\n\u001b[1;32m   1892\u001b[0m \u001b[39mif\u001b[39;00m effective_n_jobs(n_jobs) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m-> 1893\u001b[0m     \u001b[39mreturn\u001b[39;00m func(X, Y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m   1895\u001b[0m \u001b[39m# enforce a threading backend to prevent data communication overhead\u001b[39;00m\n\u001b[1;32m   1896\u001b[0m fd \u001b[39m=\u001b[39m delayed(_dist_wrapper)\n",
      "File \u001b[0;32m~/miniconda3/envs/quera/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:186\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m global_skip_validation \u001b[39m=\u001b[39m get_config()[\u001b[39m\"\u001b[39m\u001b[39mskip_parameter_validation\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    185\u001b[0m \u001b[39mif\u001b[39;00m global_skip_validation:\n\u001b[0;32m--> 186\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    188\u001b[0m func_sig \u001b[39m=\u001b[39m signature(func)\n\u001b[1;32m    190\u001b[0m \u001b[39m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/quera/lib/python3.12/site-packages/sklearn/metrics/pairwise.py:539\u001b[0m, in \u001b[0;36mnan_euclidean_distances\u001b[0;34m(X, Y, squared, missing_values, copy)\u001b[0m\n\u001b[1;32m    537\u001b[0m present_X \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m missing_X\n\u001b[1;32m    538\u001b[0m present_Y \u001b[39m=\u001b[39m present_X \u001b[39mif\u001b[39;00m Y \u001b[39mis\u001b[39;00m X \u001b[39melse\u001b[39;00m \u001b[39m~\u001b[39mmissing_Y\n\u001b[0;32m--> 539\u001b[0m present_count \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mdot(present_X, present_Y\u001b[39m.\u001b[39;49mT)\n\u001b[1;32m    540\u001b[0m distances[present_count \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mnan\n\u001b[1;32m    541\u001b[0m \u001b[39m# avoid divide by zero\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "filtered_df[:] = imputer.fit_transform(filtered_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from fancyimpute import IterativeImputer\n",
    "\n",
    "imputer = IterativeImputer()\n",
    "filtered_df[:] = imputer.fit_transform(filtered_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (quera)",
   "language": "python",
   "name": "quera"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
